<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="Blither Boom"><meta name="copyright" content="Blither Boom"><meta name="generator" content="Hexo 6.2.0"><meta name="theme" content="hexo-theme-yun"><title>Introduction-to-Probability | Guo_Yun</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...true?.options,
  });
});</script><link rel="icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"blitherboom812.github.io","root":"/","title":["Êë∏","üêü","‰∫∫","ÁöÑ","Êó•","Â∏∏"],"version":"1.10.11","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}.","hits":"${hits} results found","hits_time":"${hits} results found in ${time} ms"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap"><meta name="description" content="IntroductionProbability SpaceProbability space is a triple $(\Omega, \mathcal{F}, \mathbf{P})$, comprised of the following threeelements: 1 Sample space $\Omega$: the set of all possible outcomes of a">
<meta property="og:type" content="article">
<meta property="og:title" content="Introduction-to-Probability">
<meta property="og:url" content="https://blitherboom812.github.io/2023/02/20/Introduction-to-Probability/index.html">
<meta property="og:site_name" content="Guo_Yun">
<meta property="og:description" content="IntroductionProbability SpaceProbability space is a triple $(\Omega, \mathcal{F}, \mathbf{P})$, comprised of the following threeelements: 1 Sample space $\Omega$: the set of all possible outcomes of a">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://blitherboom812.github.io/images/prob/L2_1.jpg">
<meta property="og:image" content="https://blitherboom812.github.io/images/prob/L6_1.jpg">
<meta property="og:image" content="https://blitherboom812.github.io/images/prob/L14_1.jpg">
<meta property="og:image" content="https://blitherboom812.github.io/images/prob/L15_1.jpg">
<meta property="article:published_time" content="2023-02-20T13:05:41.000Z">
<meta property="article:modified_time" content="2023-06-18T10:31:23.000Z">
<meta property="article:author" content="Blither Boom">
<meta property="article:tag" content="note">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blitherboom812.github.io/images/prob/L2_1.jpg"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script><!-- hexo injector head_end start --><link rel="stylesheet" type="text/css" href="/css/latex.css"><link rel="stylesheet" type="text/css" href="/css/fonts.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="Table of Contents"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="Overview"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="Blither Boom"><img width="96" loading="lazy" src="/images/avatar.jpg" alt="Blither Boom"></a><div class="site-author-name"><a href="/about/">Blither Boom</a></div><span class="site-name">Guo_Yun</span><sub class="site-subtitle"></sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="Home"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="Archives"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">32</span></a></div><div class="site-state-item"><a href="/categories/" title="Categories"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">0</span></a></div><div class="site-state-item"><a href="/tags/" title="Tags"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">7</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="ÊñáÊ°£"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="ÊàëÁöÑÂ∞è‰ºô‰º¥‰ª¨" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Probability-Space"><span class="toc-number">2.</span> <span class="toc-text">Probability Space</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Sample-space"><span class="toc-number">2.1.</span> <span class="toc-text">Sample space</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sigma-algegra"><span class="toc-number">2.2.</span> <span class="toc-text">$\sigma$-algegra</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Borel-field"><span class="toc-number">2.3.</span> <span class="toc-text">Borel field</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Uncountable"><span class="toc-number">2.4.</span> <span class="toc-text">Uncountable</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Probability-measures"><span class="toc-number">2.5.</span> <span class="toc-text">Probability measures</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Discrete-models"><span class="toc-number">2.6.</span> <span class="toc-text">Discrete models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Continuous-Models"><span class="toc-number">2.7.</span> <span class="toc-text">Continuous Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Some-properties-of-Probability-measure"><span class="toc-number">2.8.</span> <span class="toc-text">Some properties of Probability measure</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conditional-Probability"><span class="toc-number">2.9.</span> <span class="toc-text">Conditional Probability</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Total-probability-theorem"><span class="toc-number">2.10.</span> <span class="toc-text">Total probability theorem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Inference-and-Bayes%E2%80%99-rule"><span class="toc-number">2.11.</span> <span class="toc-text">Inference and Bayes‚Äô rule</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Independence"><span class="toc-number">2.12.</span> <span class="toc-text">Independence</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Independence-of-two-disjoint-events"><span class="toc-number">2.12.1.</span> <span class="toc-text">Independence of two disjoint events</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Conditional-independence"><span class="toc-number">2.12.2.</span> <span class="toc-text">Conditional independence</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Discrete-Random-Variables"><span class="toc-number">3.</span> <span class="toc-text">Discrete Random Variables</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Definition"><span class="toc-number">3.1.</span> <span class="toc-text">Definition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Expectation-and-Variance"><span class="toc-number">3.2.</span> <span class="toc-text">Expectation and Variance</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conditional"><span class="toc-number">3.3.</span> <span class="toc-text">Conditional</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multiple-discrete-random-variables"><span class="toc-number">3.4.</span> <span class="toc-text">Multiple discrete random variables</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Independence-1"><span class="toc-number">3.5.</span> <span class="toc-text">Independence</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Continuous-Random-Variables"><span class="toc-number">4.</span> <span class="toc-text">Continuous Random Variables</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Probability-Density-Function"><span class="toc-number">4.1.</span> <span class="toc-text">Probability Density Function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Common-Example-for-PDF"><span class="toc-number">4.2.</span> <span class="toc-text">Common Example for PDF</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cumulative-Distribution-Functions"><span class="toc-number">4.3.</span> <span class="toc-text">Cumulative Distribution Functions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Examples-for-CDF"><span class="toc-number">4.4.</span> <span class="toc-text">Examples for CDF</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Normal-Random-Variables"><span class="toc-number">4.5.</span> <span class="toc-text">Normal Random Variables</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multiple-Continuous-Random-Variables"><span class="toc-number">4.6.</span> <span class="toc-text">Multiple Continuous Random Variables</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conditional-and-Independence"><span class="toc-number">4.7.</span> <span class="toc-text">Conditional and Independence</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#The-continuous-Bayes%E2%80%99s-rule"><span class="toc-number">4.8.</span> <span class="toc-text">The continuous Bayes‚Äôs rule</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Derived-distributions-and-Entropy"><span class="toc-number">5.</span> <span class="toc-text">Derived distributions and Entropy</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Derived-Distribution"><span class="toc-number">5.1.</span> <span class="toc-text">Derived Distribution</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Entropy"><span class="toc-number">5.2.</span> <span class="toc-text">Entropy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Maximum-entropy-distributions"><span class="toc-number">5.3.</span> <span class="toc-text">Maximum entropy distributions</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Convolution-covariance-correlation-and-conditional-expectation"><span class="toc-number">6.</span> <span class="toc-text">Convolution, covariance, correlation, and conditional expectation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Convolution"><span class="toc-number">6.1.</span> <span class="toc-text">Convolution</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Covariance-and-Correlation"><span class="toc-number">6.2.</span> <span class="toc-text">Covariance and Correlation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conditional-expectation"><span class="toc-number">6.3.</span> <span class="toc-text">Conditional expectation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conditional-expectation-as-an-estimator"><span class="toc-number">6.4.</span> <span class="toc-text">Conditional expectation as an estimator</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transforms-and-sum-of-a-random-number-of-random-variables"><span class="toc-number">7.</span> <span class="toc-text">Transforms and sum of a random number of random variables</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Properties"><span class="toc-number">7.1.</span> <span class="toc-text">Properties</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Inversion-of-transforms"><span class="toc-number">7.2.</span> <span class="toc-text">Inversion of transforms</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transform-of-Mixture-of-Distributions"><span class="toc-number">7.3.</span> <span class="toc-text">Transform of Mixture of Distributions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sum-of-independend-RVs"><span class="toc-number">7.4.</span> <span class="toc-text">Sum of independend RVs</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Weak-law-of-large-numbers"><span class="toc-number">8.</span> <span class="toc-text">Weak law of large numbers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Markov-inequality"><span class="toc-number">8.1.</span> <span class="toc-text">Markov inequality</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Chebyshev%E2%80%99s-Inequality"><span class="toc-number">8.2.</span> <span class="toc-text">Chebyshev‚Äôs Inequality</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Weak-law-of-large-numbers-1"><span class="toc-number">8.3.</span> <span class="toc-text">Weak law of large numbers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Convergence-%E2%80%9Cin-Probability%E2%80%9D"><span class="toc-number">8.4.</span> <span class="toc-text">Convergence ‚Äúin Probability‚Äù</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Many-types-of-convergence"><span class="toc-number">8.5.</span> <span class="toc-text">Many types of convergence</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Central-Limit-Theorem"><span class="toc-number">9.</span> <span class="toc-text">Central Limit Theorem</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Theorem"><span class="toc-number">9.1.</span> <span class="toc-text">Theorem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Normal-Approximation-Based-on-the-Central-Limit-Theorem"><span class="toc-number">9.2.</span> <span class="toc-text">Normal Approximation Based on the Central Limit Theorem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Proof"><span class="toc-number">9.3.</span> <span class="toc-text">Proof</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Strong-Law-of-Large-Numbers"><span class="toc-number">10.</span> <span class="toc-text">The Strong Law of Large Numbers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Theorem-1"><span class="toc-number">10.1.</span> <span class="toc-text">Theorem</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Borel-Cantelli-lemma-amp-Bernoulli-Process"><span class="toc-number">11.</span> <span class="toc-text">Borel-Cantelli lemma &amp; Bernoulli Process</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Limit-of-set-sequence"><span class="toc-number">11.1.</span> <span class="toc-text">Limit of set sequence</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Borel-Cantelli-Lemma"><span class="toc-number">11.2.</span> <span class="toc-text">Borel-Cantelli Lemma</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Stochastic-process"><span class="toc-number">11.3.</span> <span class="toc-text">Stochastic process</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Bernoulli-Process"><span class="toc-number">11.4.</span> <span class="toc-text">The Bernoulli Process</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Poisson-Process"><span class="toc-number">12.</span> <span class="toc-text">The Poisson Process</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Definition-1"><span class="toc-number">12.1.</span> <span class="toc-text">Definition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bernoulli-x2F-Poisson-Relation"><span class="toc-number">12.2.</span> <span class="toc-text">Bernoulli&#x2F;Poisson Relation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PMF-of-Number-of-Arrivals-N"><span class="toc-number">12.3.</span> <span class="toc-text">PMF of Number of Arrivals $N$</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Time-T-of-the-first-arrival"><span class="toc-number">12.4.</span> <span class="toc-text">Time $T$ of the first arrival</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Interarrival-times"><span class="toc-number">12.5.</span> <span class="toc-text">Interarrival times</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Merging-Poisson-Processes"><span class="toc-number">12.6.</span> <span class="toc-text">Merging Poisson Processes</span></a></li></ol></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="https://blitherboom812.github.io/2023/02/20/Introduction-to-Probability/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="Blither Boom"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="Guo_Yun"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Introduction-to-Probability</h1><div class="post-meta"><div class="post-time"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="Created: 2023-02-20 21:05:41" itemprop="dateCreated datePublished" datetime="2023-02-20T21:05:41+08:00">2023-02-20</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-2-line"></span></span> <time title="Modified: 2023-06-18 18:31:23" itemprop="dateModified" datetime="2023-06-18T18:31:23+08:00">2023-06-18</time></div><div class="post-classify"><span class="post-tag"><a class="tag-item" href="/tags/note/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">note</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h2 id="Probability-Space"><a href="#Probability-Space" class="headerlink" title="Probability Space"></a>Probability Space</h2><p>Probability space is a triple $(\Omega, \mathcal{F}, \mathbf{P})$, comprised of the following three<br>elements:</p>
<p>1 Sample space $\Omega$: the set of all possible outcomes of an experiment</p>
<p>2 $\sigma$-algebra (or $\sigma$-field) $\mathcal F$: a collection of subsets of $\Omega$</p>
<p>3 Probability measure $\mathbf P$: a function that assigns a nonnegative<br>probability to every set in the $\sigma$-algebra $\mathcal F$</p>
<h3 id="Sample-space"><a href="#Sample-space" class="headerlink" title="Sample space"></a>Sample space</h3><p>Mutually exclusive: no identical element.</p>
<p>Collectively exhaustive: all results should be included.</p>
<h3 id="sigma-algegra"><a href="#sigma-algegra" class="headerlink" title="$\sigma$-algegra"></a>$\sigma$-algegra</h3><p>not unique</p>
<p>3 requirements:</p>
<div>$$
\varnothing \in \mathcal F\\
\forall A \in \mathcal F, A^c \in \mathcal F\\
\forall A_k \in \mathcal F, k=1, 2, ..., 
\cup_{k=1}^{\infty}A_k\in \mathcal F
$$</div>

<h3 id="Borel-field"><a href="#Borel-field" class="headerlink" title="Borel field"></a>Borel field</h3><p>used to measure intervals</p>
<p>when $\Omega$ is continuous($\R$ for example), Borel field is useful.</p>
<p>‚Äúminimum‚Äù $\sigma$-algebra means deleting any element in the $\mathcal B (\mathbf R)$ will miss the requirements.</p>
<p><img src="/../images/prob/L2_1.jpg" loading="lazy"></p>
<h3 id="Uncountable"><a href="#Uncountable" class="headerlink" title="Uncountable"></a>Uncountable</h3><p>decimal numbers between 0 and 1 are uncountable.</p>
<h3 id="Probability-measures"><a href="#Probability-measures" class="headerlink" title="Probability measures"></a>Probability measures</h3><div>$$
P:\mathcal F \rightarrow [0, 1]
$$</div>

<p><strong>Nonnegativity</strong> $P(A)\ge0, \forall A \in \mathcal{  F}$</p>
<p><strong>Normalization</strong>  $P(\empty)&#x3D;0, P(\Omega)&#x3D;1$</p>
<p><strong>Countable additivity</strong> $A_1, A_2, ‚Ä¶ \text { is disjoint in }\mathcal F, P(A_1\cup A_2\cup ‚Ä¶)&#x3D;P(A_1)+P(A_2)+‚Ä¶$</p>
<ul>
<li>They are the axioms of probability. </li>
<li>Probability is a mapping from $\sigma$-algebra to a real number betwenn 0 and 1, which intuitively specifies the ‚Äúlikelihood‚Äù of any event. </li>
<li>There exist non-measurable sets, on which we cannot define a probability measure.</li>
</ul>
<h3 id="Discrete-models"><a href="#Discrete-models" class="headerlink" title="Discrete models"></a>Discrete models</h3><div>$$
P(\lbrace s_1, ..., s_n\rbrace )=P(s_1)+...+P(s_n)\\
P(A) = \frac{\text{\# of elements of }A}{\text{total \# of elements of sample points}}
$$</div>


<h3 id="Continuous-Models"><a href="#Continuous-Models" class="headerlink" title="Continuous Models"></a>Continuous Models</h3><p>Probability &#x3D; Area</p>
<h3 id="Some-properties-of-Probability-measure"><a href="#Some-properties-of-Probability-measure" class="headerlink" title="Some properties of Probability measure"></a>Some properties of Probability measure</h3><div>$$
A\sub B\Rightarrow P(A)\le P(B)\\
P(A\cup B)=P(A)+P(B)-P(A\cap B)\\
P(A\cup B) \le P(A) + P(B)\\
P(A\cup B \cup C)=P(A) + P(A^C\cap B) + P(A^C\cap B^C\cap C)
$$</div>

<h3 id="Conditional-Probability"><a href="#Conditional-Probability" class="headerlink" title="Conditional Probability"></a>Conditional Probability</h3><div>$$
P(A|B)=\frac{P(A\cap B)}{P(B)}
$$</div>

<ul>
<li>If $P(B)&#x3D;0$, $P(A|B)$ is undefined.</li>
<li>For a fixed event $B$, $P(A|B)$ can be verified as a legitimate probability measure on the new universe. $P(A, B)\ge 0$, $P(\Omega|B)&#x3D;1$, $P(A_1\cup A_2\cup‚Ä¶|B)&#x3D;P(A_1|B)+P(A_2|B)+‚Ä¶$</li>
<li><div>$P(A|B)=\frac{\text{ \# of elements of }A\cap B}{\text{total \# of elements of }B}$</div></li>
</ul>
<h3 id="Total-probability-theorem"><a href="#Total-probability-theorem" class="headerlink" title="Total probability theorem"></a>Total probability theorem</h3><p>Let $A_1, ‚Ä¶, A_n$ be disjoint events that form a partition of the sample space and assume that $P(A_i)&gt;0$ for all $i$. Then for any event B, we have</p>
<div>$$
P(B) = \sum_{i=1}^n P(A_i\cap B) = \sum_{i=1}^nP(A_i)P(B|A_i)
$$</div>

<p><strong>Remark</strong> </p>
<ul>
<li>The definition of partition is that $\cup_{i&#x3D;1}^n A_i &#x3D; \Omega, A_i\cap A_j &#x3D; \emptyset, \forall i\ne j$</li>
<li>The probability of B is a weighted average of its conditional probability under each scenario</li>
<li>Each scenario is weighted according to its prior probability</li>
<li>Useful when $P(B|A_i)$ is known or easy to derive</li>
</ul>
<h3 id="Inference-and-Bayes‚Äô-rule"><a href="#Inference-and-Bayes‚Äô-rule" class="headerlink" title="Inference and Bayes‚Äô rule"></a>Inference and Bayes‚Äô rule</h3><p>Let $A_1, ‚Ä¶, A_2$ be disjoint events that from a partition of the sample space and assume that $P(A_i) \gt 0$  for all $i$. Then for any event $B$ such that $P(B)\gt 0$, we have </p>
<div>$$
P(A_i|B) = \frac{P(A_i)P(B|A_i)}{P(B)} = \frac{P(A_i)P(B|A_i)}{\sum_{j=1}^nP(A_j)P(B|A_j)}
$$</div>

<p><strong>Remarks</strong></p>
<ul>
<li>Relates conditional probabilities of the form $P(A_i|B)$ with conditional probabilities of the form $P(B|A_i)$</li>
<li>often used in inference: effect $B$ $\lrarr$ cause $A_i$</li>
</ul>
<p>The meaning of $P(A_i|B)$ in the view of Bayes: the belief of $A_i$ is revised if we observed effect $B$. If the cause and the effect are closely binded($P(B|A_i) &gt; P(B|A_i^c)$), then the belief $A_i$ is enhanced by the observation of effect $B$($P(A_i|B) &gt; P(A)$). This can be derived from the Bayes‚Äô rule through simple calculation. If $P(A_i|B)&#x3D;P(A_i)$, then $B$ provides no information on $A_i$.</p>
<h3 id="Independence"><a href="#Independence" class="headerlink" title="Independence"></a>Independence</h3><h4 id="Independence-of-two-disjoint-events"><a href="#Independence-of-two-disjoint-events" class="headerlink" title="Independence of two disjoint events"></a>Independence of two disjoint events</h4><p>Events A and B are called <strong>independent</strong> if </p>
<div>$$
P(A\cap B) = P(A)\cdot P(B)
$$</div>
or equivalently, when $P(B) > 0$, 

<div>$$
P(A|B) = P(A)
$$</div>

<p><strong>Remarks</strong></p>
<ul>
<li>Occurrence of B provides no information about A‚Äôs occurrence</li>
<li>Equivalence due to $P(A\cap B) &#x3D; P(B)\cdot P(A|B)$</li>
<li>Symmetric with respect to $A$ and $B$.</li>
<li><ul>
<li>applies even if $P(B) &#x3D; 0$</li>
</ul>
</li>
<li><ul>
<li>implies $P(B|A) &#x3D; P(B)$ and $P(A|B^c) &#x3D; P(A)$</li>
</ul>
</li>
<li>Does not imply that A and B are disjoint, indeed opposite!</li>
<li><ul>
<li>Two disjoint events are never independent!($P(A\cap B) &#x3D; 0$, but $P(A)\cdot P(B)\ne 0$)</li>
</ul>
</li>
</ul>
<h4 id="Conditional-independence"><a href="#Conditional-independence" class="headerlink" title="Conditional independence"></a>Conditional independence</h4><div>$$
P(A\cap B | C) = P(A| C) \cdot P(B|C)
$$</div>

<p><strong>Definition</strong></p>
<p>Event $A_1, A_2, ‚Ä¶, A_n$ are called independent if: </p>
<div>$$
P(A_i\cap A_j\cap ...\cap A_q) = P(A_1)P(A_j)...P(A_q)
$$</div>

<p>for any distinct indices $i, j, \dots q$ chosen from $\lbrace 1, \dots n\rbrace $.</p>
<p>Pairwise is independence does not imply independence.</p>
<h2 id="Discrete-Random-Variables"><a href="#Discrete-Random-Variables" class="headerlink" title="Discrete Random Variables"></a>Discrete Random Variables</h2><p>Random Variable is neither random, nor variable.</p>
<h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>We care about the probability that $X \le x$ instead $X &#x3D; x$ in the consideration of generality. </p>
<p><strong>Random variables</strong></p>
<p>Given a probability space $(\Omega, F, P)$, a random variable is a function $X: \Omega \rightarrow \R$ with the probability that $\lbrace \omega \in \Omega: X(\omega) \le x\rbrace  \in \mathcal F$ for each $x\in \R$. Such a function $X$ is said to be $\mathcal F$-measurable.</p>
<p><strong>Probability Mass Function(PMF)</strong></p>
<div>$$
p_X(x)=P(X=x)=P(\lbrace \omega \in \Omega \text{ s.t. } X(\omega)=x\rbrace )
$$</div>

<p>Bonulli PMF: </p>
<div>$$ 
p_X(k) = \begin{cases}
    p, &\text{if } k = 1\\
    1-p, &\text{if }k=0
\end{cases}
$$</div>

<p>Binomial PMF: $p_X(k)&#x3D;\binom{n}{k}p^k(1-p)^{n-k}$</p>
<p>Geometric PMF: $p_X(k)&#x3D;(1-p)^{k-1}p$</p>
<p>Poisson PMF: $p_X(k)&#x3D;e^{-\lambda}\frac{\lambda^k}{k!}$. Note: $\sum_{k&#x3D;0}^\infty e^{-\lambda}\frac{\lambda^k}{k!}&#x3D;e^{-\lambda}e^\lambda&#x3D;1$</p>
<p>If $y&#x3D;g(x)$, $p_Y(y)&#x3D;\sum_{\lbrace x|g(x)&#x3D;y \rbrace} p(x)$.</p>
<h3 id="Expectation-and-Variance"><a href="#Expectation-and-Variance" class="headerlink" title="Expectation and Variance"></a>Expectation and Variance</h3><p><strong>Expectation</strong></p>
<div>$$
E[X] = \sum_x xp_X(x)
$$</div>

<p>Note: we assume that the sum converges.</p>
<p>Properties:</p>
<div>$$
E[Y]=\sum_x g(x)p_X(x)\\
E[\alpha X + \beta] = \alpha E[X] + \beta
$$</div>

<p><strong>Variance</strong></p>
<div>$$
\text{var}(X) = E \left[(X-E[X])^2\right]=\sum_x (x-E[X])^2 p_X(x)
$$</div>

<p>Standard deviation: $\sigma_X&#x3D;\sqrt{\text{var}(X)}$</p>
<p>Properties: </p>
<div>$$
\text{var}(X) = E[X^2] -(E[X])^2\\
\text{var}(X)\ge 0\\
\text{var}(\alpha X + \beta) = \alpha^2\text{var} (X)
$$</div>

<p><strong>Bernoulli RV</strong></p>
<div>$$
p_X(k) = \begin{cases}
    p, &\text{if } k = 1\\
    1-p, &\text{if }k=0
\end{cases}\\
E[X] = p\\
E[X^2] = p\\
\text{var}(X) = p(1-p)
$$</div>

<p><strong>Discrete Uniform RV</strong></p>
<div>$$
p_X(k) = \begin{cases}
    \frac {1}{b-a+1}, &\text{if } k = a, a+1, ..., b\\
    0, &\text{otherwise}
\end{cases}\\
E[X] = \frac{a+b}{2}\\
\text{var}(X) = \frac{(b-a)(b-a+2)}{12}
$$</div>

<p><strong>Poisson RV</strong></p>
<div>$$
p_X(k)=e^{-\lambda}\frac{\lambda^k}{k!}\\
E[X] = \lambda\\
\text{var}(X)=\lambda
$$</div>

<h3 id="Conditional"><a href="#Conditional" class="headerlink" title="Conditional"></a>Conditional</h3><div>$$
p_{X|A(x)} = P(X=x|A) = \frac{P(\lbrace X=x\rbrace \cap A)}{P(A)}
$$</div>

<div>$$
\sum_x p_{X|A}(x) = 1
$$</div>

<div>$$
E[X|Y=y] = \sum_x xp_{X|Y}(x|y)\\
E[g(X)|Y=y] = \sum_x g(x)p_{X|Y}(x|y)
$$</div>

<p><strong>Total expectation theorem</strong></p>
<p>$A_1, \dots, A_n$ is a partition of sample space</p>
<div>$$
P(B) = P(A_1)P(B|A_1) + \dotsb + P(A_n)P(B|A_n)\\
p_X(x) = P(A_1)p_{X|A_1}(x) + \dotsb + P(A_n)p_{X|A_n}(x)\\
E[X] = P(A_1)E[X|A_1] + \dotsb + P(A_n)E[X|A_n]
$$</div>

<p>We derive the expectation and variance use the theories above.</p>
<p><strong>Geometric PMF example</strong></p>
<div>$$
p_X(k) = (1-p)^{k-1}p, k = 1, 2, \dots\\
E[X] = \sum_{k=1}^\infty kp_X(k) = \sum_{k=1}^\infty k(1-p)^{k-1}p\\
E[X^2] = \sum_{k=1}^\infty k^2p_X(k) = \sum_{k=1}^\infty k^2(1-p)^{k-1}p\\
\text {var}(X) = E[X^2] - (E[X])^2
$$</div>

<p>However, the Geometric has a memoryless property.</p>
<div>$$
p_{X|X>1}(k) = \frac{P(\lbrace X>1\rbrace \cap \{X=k\})}{P(X>1)} = \frac{(1-p)^{k-1}p}{1-p} = (1-p)^{k-2}p
$$</div>

<p>Thus, </p>
<div>$$
E[X] = P(X=1)E[X|X=1] + P(X>1)E[X|X>1]=p+(1-p)(E[1 + X])\\
\Rightarrow E[X] = 1/p\\
E[X^2] = P(X=1)E[X^2|X=1] + P(X>1)E[X^2|X>1] = p + (1-p)E[(1+X)^2]=p + (1-p)(1+2E[X]+E[X^2])\\
\Rightarrow E[X^2] = \frac{2-p}{p^2}\\
\Rightarrow\text{var} (X) = \frac{1-p}{p^2}
$$</div>

<h3 id="Multiple-discrete-random-variables"><a href="#Multiple-discrete-random-variables" class="headerlink" title="Multiple discrete random variables"></a>Multiple discrete random variables</h3><p><strong>Joint PMFs</strong></p>
<div>$$
p_{X, Y}(x, y) = P(X = x, Y= y) = P(\lbrace X(\omega) = x\rbrace \cap \{Y(\omega) = y\})
$$</div>

<div>$$
\sum_x\sum_y p_{X, Y}(x, y) = 1
$$</div>

<p><strong>Marginal PMF</strong></p>
<div>$$
p_X(x) = \sum_y P(X=x, Y=y) = \sum_y p_{X, Y}(x, y)
$$</div>

<p><strong>Conditional PMF</strong></p>
<div>$$
p_{X|Y}(x|y) = P(X = x | Y = y) = \frac{p_{X, Y}(x, y)}{p_Y(y)}
$$</div>

<div>$$
\sum_x p_{X|Y}(x|y) = 1
$$</div>

<p><strong>Funcitons of multiple RVs</strong></p>
<div>$$
Z = g(X, Y)\\
p_Z(z) = \sum_{\lbrace (x, y)|g(x, y)=z \rbrace  } p_{X, Y}(x, y)
$$</div>

<p><strong>Expectations</strong></p>
<div>$$
E[g(X, Y)] = \sum_x\sum_y g(x, y)p(X, Y)(x, y)\\
E[g(X, Y, Z)] = \sum_x\sum_y\sum_z g(x, y, z)p(X, Y, Z)(x, y, z)
$$</div>

<div>$$
E[g(X,  Y)] \not\equiv g(E[X], E[Y])
$$</div>

<p><strong>linearity</strong></p>
<div>$$
E[\alpha X + \beta] = \alpha E[X] + \beta\\
E[X + Y + Z] = E[X] + E[Y] + E[Z]
$$</div>

<p>Let‚Äôs calculate the Mean of Binominal RV.</p>
<div>$$
X_i=
\begin{cases}
    1, &\text{if success in trial } i,\\
    0, & \text{otherwise.}
\end{cases}\\
X = X_1 + X_2 + \dotsb X_n\\
E[X] = \sum_{i = 1}^n E[X_i] = np\\
\text{var}(X) = np(1-p)
$$</div>

<h3 id="Independence-1"><a href="#Independence-1" class="headerlink" title="Independence"></a>Independence</h3><p><strong>Independence</strong></p>
<div>$$
p_{X, Y}(x, y) = p_X(x) \cdot p_Y(y)
$$</div>

<p>if $X$ and $Y$ are independent:</p>
<div>$$
E[XY] = E[X]E[Y]\\
E[g(X)h(Y)] = E[g(X)]E[h(Y)]\\
\text{var}(X + Y) = \text{var}(X) + \text{var}(Y)
$$</div>

<p><strong>Conditional independence</strong></p>
<div>$$
p_{X, Y|A}(X, Y) = p_{X|A}(x) \cdot p_{Y|A}(y)
$$</div>

<h2 id="Continuous-Random-Variables"><a href="#Continuous-Random-Variables" class="headerlink" title="Continuous Random Variables"></a>Continuous Random Variables</h2><h3 id="Probability-Density-Function"><a href="#Probability-Density-Function" class="headerlink" title="Probability Density Function"></a>Probability Density Function</h3><ul>
<li>$f_X(x)\ge 0\text{ for all }x$</li>
<li>$\int_{-\infty}^\infty f_X(x)\mathrm dx &#x3D; 1$</li>
<li>If $\delta$ is very small, then $P([x, x+\delta])\approx f_X(x) \cdot \delta$</li>
<li>For any subset $B$ of the real line, $P(X\in B) &#x3D; \int_B f_X(x)\mathrm d x$.</li>
</ul>
<p><strong>Expectation</strong></p>
<div>$$
E[X] = \int_{-\infty}^\infty xf_X(x)\mathrm dx\\
E[g(x)] = \int_{-\infty}^\infty g(x)f_X(x)\mathrm dx
$$</div>

<p>Assuming that the integration is well-defined. The Cauchy distribution ($\frac{1}{1+x^2}$)doesn‚Äôt have expectation since $\frac{x}{1+x^2}$ is not absolutely integrably.</p>
<p><strong>Variance</strong></p>
<div>$$
\text{var}(X) = E[(X - E[X])^2] = \int_{-\infty}^\infty(x - E[x])^2 f_X(x)\mathrm dx\\
0\le \text{var}(x) = E[X^2] - (E[X])^2
$$</div>

<p><strong>Uniform RV</strong></p>
<div>$$
f_X(x) = \begin{cases}
    \frac{1}{b-a}, &\text{if }a\le x\le b,\\
    0, &\text{otherwise.}
\end{cases}
$$</div>

<div>$$
E[X] = \frac{a+b}{2}\\
E[X^2] = \frac{a^2+b^2 + ab}{3}\\
\text{var}(X) = \frac{(b-a)^2}{12}
$$</div>


<p>Properties:</p>
<div>$$
E[aX+b] = aE[X] + b\\
\text{var}(aX+b) = a^2\text{var}(X)
$$</div>

<h3 id="Common-Example-for-PDF"><a href="#Common-Example-for-PDF" class="headerlink" title="Common Example for PDF"></a>Common Example for PDF</h3><p><strong>Exponential Random Variable</strong></p>
<div>$$
f_X(x) = \begin{cases}
    \lambda e^{-\lambda x}, &\text{if }x \ge 0,\\
    0, &\text{otherwise.}
\end{cases}
$$</div>

<div>$$
P(X\ge a) = e^{-\lambda a}\\
E[X] = \frac{1}{\lambda}\\
\text{var}(X) = \frac{1}{\lambda^2}
$$</div>

<h3 id="Cumulative-Distribution-Functions"><a href="#Cumulative-Distribution-Functions" class="headerlink" title="Cumulative Distribution Functions"></a>Cumulative Distribution Functions</h3><div>$$
F_X(x) = P(X\le x) = \begin{cases}
    \sum_{k\le x}p_X(k), &\text{if } X \text{ is discrete,}\\
    \int_{-\infty}^x f_X(t)\mathrm dt, &\text{if } X \text{ is continuous.}
\end{cases}
$$</div>

<p><strong>Properties</strong></p>
<div>$$
\text{if } x \le y, \text{then } F_X(x)\le F_X(y).\\
F_X(x)\text{ tends to 0 as } x \rightarrow -\infty, \text{and to 1 as} x \rightarrow \infty\\
\text{If } X \text{ is discrete, then } F_X(x) \text{ is a piecewise constant function of }x.\\
\text{If } X \text{ is continuous, then } F_X(x) \text{is a continuous funciton of }x.\\
\text{If } X \text{ is discrete and takes integer values, the PMF and the CDF can be obtained from each other by summing or differcing: }\\
F_X(k) = \sum_{i = -\infty}^k p_X(i),\\
p_X(k) = P(X\le k) - P(X \le k -1) = F_X(k) - F_X(k - 1),\\
\text{ for all integers }k.\\
\text{If } X \text{ is continuous, the PDF and the CDF can be obtained from each other by integration or differentiation: }\\
F_X(x) = \int_{-\infty}^x f_X(t)\mathrm dt, f_X(x) = \frac{\mathrm dF_X}{\mathrm dx}(x)
$$</div>

<h3 id="Examples-for-CDF"><a href="#Examples-for-CDF" class="headerlink" title="Examples for CDF"></a>Examples for CDF</h3><p><strong>Geometric CDF</strong></p>
<div>$$
F_{\text{geo}}(n) = \sum_{k = 1}^n p(1-p)^{k-1} = 1-(1-p)^n, \text{for } n = 1, 2, \dots
$$</div>

<p><strong>Exponential CDF</strong></p>
<div>$$
F_{\text{exp}}(x) = P(X\le x) = 0, \text{ for } x\le0,\\
F_{\text{exp}}(x) = \int_{0}^x \lambda e^{-\lambda t}\mathrm dt = 1 - e^{-\lambda x}, \text{for }x\ge 0.
$$</div>

<p>Exponential Distribution is Memoriless, like Geometric: </p>
<div>$$
P(X \ge c + x| X \ge c) = e^{-\lambda x} = P(X \ge x)\\
$$</div>

<p>The relationship: </p>
<p><img src="/../images/prob/L6_1.jpg" loading="lazy"></p>
<h3 id="Normal-Random-Variables"><a href="#Normal-Random-Variables" class="headerlink" title="Normal Random Variables"></a>Normal Random Variables</h3><div>$$
f_X(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-(x-\mu)^2/2\sigma^2}\\
E[X] =\mu\\
\text{var}(X) = \sigma^2
$$</div>

<p>Gaussian is good, since adding two Gaussian functions resulting in a new Gaussian functions. And with a huge mount of samples, the distribution is close to Gaussian(Central limit theorem).</p>
<p><strong>The Standard Normal Random Variable</strong></p>
<p>Normal(Gaussian)</p>
<div>$$
Y = \frac{X - \mu}{\sigma}\\
f_Y(y) = \frac{1}{\sqrt{2\pi}}e^{-y^2/2}\\
E[Y] = 0\\
\text{var}(Y) = 1\\
$$</div>

<p>The CDF of Normal Random Variable $\Phi(y)$ can not be derived directly, we can use the standard normal table to get the value.</p>
<div>$$
\Phi(-y) = 1 - \Phi(y)
$$</div>

<h3 id="Multiple-Continuous-Random-Variables"><a href="#Multiple-Continuous-Random-Variables" class="headerlink" title="Multiple Continuous Random Variables"></a>Multiple Continuous Random Variables</h3><p><strong>Joint PDFs</strong></p>
<p>The two continuous RVs X and Y, with the same experiment, are jointly continuous if they can be described by a joint PDF $f_{X, Y}$, where $f_{X, Y}$ is a nonnegative function that satisfies </p>
<div>$$
P((X, Y) \in B) = \iint_{(x, y)\in B} f(X, Y)\mathrm d x\mathrm dy
$$</div>

<p>for every subset B of the two-dimensional plane. In particular, when B is the form $B &#x3D; \lbrace (x, y)|a\le x \le b, c\le y \le d\rbrace $, we have</p>
<div>$$
P(a\le X \le b, c \le Y \le d) = \int_c^d\int_a^bf_{X, Y}(x, y)\mathrm dx\mathrm dy
$$</div>

<p><strong>Normalization</strong> </p>
<div>$$
\int_{-\infty}^\infty\int_{-\infty}^\infty f_{X, Y}(x, y)\mathrm dx\mathrm dy = 1
$$</div>

<p><strong>Interpretation(Small rectangle)</strong></p>
<div>$$
P(a\le X \le a + \delta, c \le Y \le c + \delta) \approx f_{X, Y}(a, c)\cdot\delta^2
$$</div>

<p><strong>Marginal PDF</strong></p>
<div>$$
P(X\in A) = P(X \in A, Y \in (-\infty, \infty)) = \int_A \int_{-\infty}^\infty f_{X, Y}(x, y)\mathrm dy\mathrm dx
$$</div>

<div>$$
f_X(x) = \int_{-\infty}^\infty f_{X, Y}(x, y)\mathrm dy\\
f_Y(y) = \int_{-\infty}^\infty f_{X, Y}(x, y)\mathrm dx
$$</div>

<p><strong>Joint CDF</strong></p>
<p>If X and Y are two RVs asscociated with the same experiment, then the joint CDF of X and Y is the function</p>
<div>$$
F_{X, Y}(x, y) = P(X\le x, Y\le y) = P(X\le x|Y\le y)P(Y\le y) = \int_{-\infty}^y\int_{-\infty}^x f_{X, Y}(u, v)\mathrm du\mathrm dv
$$</div>

<p>Conversely</p>
<div>$$
f_{X, Y}(x, y) = \frac{\partial^2F_{X, Y}}{\partial x\partial y}(x, y)
$$</div>

<p><strong>Expectations</strong></p>
<div>$$
E[g(X, Y)] = \int_{-\infty}^\infty\int_{-\infty}^\infty g(x, y)f_{X, Y}(x, y)\mathrm dx\mathrm dy
$$</div>

<p>If g is linear, of the form of $g(x, y) &#x3D; ax + by + c$, then</p>
<div>$$
E[g(X, Y)] = aE[X] + bE[Y] + c
$$</div>

<p>X and Y are called independent if </p>
<div>$$
f_{X, Y}(x, y) = f_X(x)f_Y(y)
$$</div>

<h3 id="Conditional-and-Independence"><a href="#Conditional-and-Independence" class="headerlink" title="Conditional and Independence"></a>Conditional and Independence</h3><p><strong>Conditional PDFs</strong></p>
<p>Let X and Y be continuous RVs with joint PDF $f_{X, Y}$. For any $f_Y(y) \gt 0$, the conditional PDF of X given Y &#x3D; y is defined by</p>
<div>$$
f_{X|Y}(x|y) = \frac{f_{X, Y}(x, y)}{f_Y(y)}
$$</div>

<p>Discrete case: </p>
<div>$$
p_{X|Y}(x|y) = \frac{p_{X, Y}(x, y)}{p_Y(y)}
$$</div>

<p>By analogy, for fixed y would like: </p>
<div>$$
P(x \le X \le x + \delta|Y = y) \approx f_{X|Y}(x|y)\cdot\delta
$$</div>

<p>But {Y &#x3D; y} is a zero-probability event.</p>
<p>Let $B &#x3D; \lbrace y\le Y \le y + \epsilon\rbrace $, for small $\epsilon &gt; 0$. Then</p>
<div>$$
P(x \le X \le x + \delta|Y \in B) \approx \frac{P(x \le X \le x + \delta)}{P(y \le Y \le y + \epsilon)} \approx \frac{f_{X, Y}(x, y)\cdot\epsilon\delta}{f_Y(y)\cdot\epsilon} \approx f_{X|Y}(x|y)\cdot\delta
$$</div>

<p>Limiting case when $\epsilon \rightarrow 0$, to define conditional PDF where the denominator is a zero-probability event.</p>
<p><strong>Conditional Expectation</strong></p>
<p>The conditional expectation of X given that A has happened is defined by </p>
<div>$$
E[X|A] = \int_{-\infty}^\infty xf_{X|A}(x)\mathrm dx
$$</div>

<p>For a function g, we have</p>
<div>$$
E[g(X)|A] = \int_{-\infty}^\infty g(x)f_{X|A}(x)\mathrm dx
$$</div>

<p><strong>Total expectation theorem</strong></p>
<p>Le $A_1, A_2, \dots A_n$ be disjoint events that form a partition of the sample space $\Omega$. And $P(A_i)\gt 0$ for all $i$. Then</p>
<div>$$
E[g(X)] = \sum_{i=1}^n P(A_i)E[g(X)|A_i]
$$</div>

<p>Conditional Expectation</p>
<p>The conditional expectation of X given that $Y &#x3D; y$ has happened is defined by </p>
<div>$$
E[X|Y=y] = \int_{-\infty}^\infty xf_{X|Y}(x|y)\mathrm dx
$$</div>

<p>For a function g, we have</p>
<div>$$
E[g(X)|Y=y] = \int_{-\infty}^\infty g(x)f_{X|Y}(x|y)\mathrm dx
$$</div>

<p>Total expectation theorem</p>
<div>$$
E[X] = E_{Y}\left[E_{X|Y}[X|Y]\right] = \int_{-\infty}^\infty E[X|Y = y]f_Y(y)\mathrm dy
$$</div>

<p><strong>Independence</strong></p>
<p>Two continuous RVs $X$ and $Y$ are independent if and only if</p>
<div>$$
f_{X, Y}(x, y) = f_X(x)f_Y(y)
$$</div>

<p>Independence is the same as the condition</p>
<div>$$
f_{X|Y}(x|y) = f_X(x)
$$</div>

<p>If $X$ and $Y$ are independent, then</p>
<div>$$
E[XY] = E[X]E[Y]\\
E[g(x)h(y)] = E[g(x)]E[h(y)], \forall g, h\\
\text{var}(X + Y) = \text{var}(X) + \text{var}(Y)\\
$$</div>

<h3 id="The-continuous-Bayes‚Äôs-rule"><a href="#The-continuous-Bayes‚Äôs-rule" class="headerlink" title="The continuous Bayes‚Äôs rule"></a>The continuous Bayes‚Äôs rule</h3><div>$$
f_{Y|X}(y|x) = \frac{f_Y(y)f_{X|Y}(x|y)}{f_Y(y)}
$$</div>

<p>Based on the normalization property $\int_{-\infty}^\infty f_{X|Y}(x|y)\mathrm dx &#x3D; 1$,</p>
<div>$$
f_{Y|X}(y|x) = \frac{f_Y(y)f_{X|Y}(x|y)}{\int_{-\infty}^\infty f_X(t)f_{Y|X}(y|t)\mathrm dt}
$$</div>

<h2 id="Derived-distributions-and-Entropy"><a href="#Derived-distributions-and-Entropy" class="headerlink" title="Derived distributions and Entropy"></a>Derived distributions and Entropy</h2><h3 id="Derived-Distribution"><a href="#Derived-Distribution" class="headerlink" title="Derived Distribution"></a>Derived Distribution</h3><p>If we want to calculate the expectation $E[g(X)]$, there‚Äôs no need to calculate the PDF $f_X$ of $X$.</p>
<p>But sometimes we want the PDF $f_Y$ of $Y &#x3D; g(X)$, where $Y$ is a new RV.</p>
<p><strong>Principal Method</strong></p>
<p>Two-step procedure for the calculation of the PDF of a function $Y&#x3D;g(X)$ of a continuous RV $X$</p>
<ol>
<li>Calcualte the CDF $F_Y$ of $Y$: $F_Y(y) &#x3D; P(Y \le y)$</li>
<li>Differentiate $F_Y$ to obtain the PDF $f_Y$ of $Y$: $f_Y(y) &#x3D; \frac{\mathrm d F_Y}{\mathrm d y}(y)$</li>
</ol>
<p><strong>The PDF of $Y&#x3D;aX + b$</strong></p>
<p>Suppose $a&gt;0$ and $b$ are constants.</p>
<div>$$
f_Y(y) = \frac{\mathrm d F_Y}{\mathrm d y}(y) = \frac{\mathrm d}{\mathrm d y} F_X(\frac{y-b}{a}) = \frac{1}{a}f_X(\frac{y-b}{a})
$$</div>

<p>If $X$ is Normal, then $Y &#x3D; aX + b$ is also Normal.</p>
<p>Suppose X is normal with mean $\mu$ and variance $\sigma^2$. Then</p>
<div>$$
f_Y(y) = \frac{1}{a\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y-b-a\mu)^2}{2a^2\sigma^2}\right)
$$</div>

<div>$$
Y = aX + b \sim N(a\mu + b, a^2\sigma^2)
$$</div>

<p><strong>The PDF of a strictly monotonic function</strong></p>
<p>Suppose $g$ is a strictly monotonic function and that for some function $h$ and all $x$ in the range of $X$ we have </p>
<div>$$
y = g(x) \text{ if and only if } x = h(y)
$$</div>

<p>Assume that $h$ is differentiable.</p>
<p>Then the PDF of $Y &#x3D; g(X)$ is given by</p>
<div>$$
f_Y(y) = \frac{\mathrm d F_Y}{\mathrm d y}(y) = \frac{\mathrm d}{\mathrm d y} F_X(h(y)) = f_X(h(y))\left|\frac{\mathrm d h}{\mathrm d y}(y)\right|
$$</div>

<h3 id="Entropy"><a href="#Entropy" class="headerlink" title="Entropy"></a>Entropy</h3><p><strong>Defintion</strong></p>
<p>Discrete case</p>
<p>Let $X$ be a discrete RV defined on probability space $(\Omega, \mathcal F, P)$. The <strong>entropy</strong> of $X$ is defined by</p>
<div>$$
H(X) = -E[\ln p_X(X)] = -\sum_{k} p_X(x_k)\ln p_X(x_k)
$$</div>

<p>Continuous case</p>
<p>Let $X$ be a continuous RV defined on probability space $(\Omega, \mathcal F, P)$. The <strong>differential entropy</strong> of $X$ is defined by</p>
<div>$$
H(X) = -E[\ln f_X(X)] = -\int_{-\infty}^\infty f_X(x)\ln f_X(x)\mathrm dx
$$</div>


<p><strong>Remarks</strong></p>
<ul>
<li>a special expectation of a random variable</li>
<li>a measure of uncertainty in a random experiment</li>
<li><ul>
<li>the larger the entropy, the more uncertain the experiment</li>
</ul>
</li>
<li><ul>
<li>For a deterministic event, the entropy is zero</li>
</ul>
</li>
<li>The base of logarithm can be different. Changing the base od the logarithm is equivalent to multiplying the entropy by a constant.</li>
<li><ul>
<li>With base 2, we say that the entropy is in units of <strong>bits</strong></li>
</ul>
</li>
<li><ul>
<li>With base e, we say that the entropy is in units of <strong>nats</strong></li>
</ul>
</li>
<li>The basis of information theory</li>
</ul>
<h3 id="Maximum-entropy-distributions"><a href="#Maximum-entropy-distributions" class="headerlink" title="Maximum entropy distributions"></a>Maximum entropy distributions</h3><p>‚Ä¢ Maximum entropy distributions</p>
<p>‚àí Distributions with maximum entropy under some constraints</p>
<p>‚àí Gaussian, exponential, and uniform distributions are all maximum entropy distributions under certain conditions</p>
<p>‚Ä¢ Why studying maximum entropy distributions?</p>
<p>‚àí The most random distribution, reflecting the maximum uncertainty about the quantity of interest</p>
<p><strong>Definition</strong></p>
<p><strong>Discrete Case</strong></p>
<p>X can be a finite number of values $x_1, x_2, \dots, x_n$, satisfying $p_X(x_k) &#x3D; p_k.$</p>
<p>We have the following optimization problem:</p>
<div>$$
\max_{X} H(X) = \max_{p_1, p_2, \dots, p_n} \left(-\sum_{k=1}^n p_k\ln p_k\right)\\
\text{s.t.} \sum_{k=1}^n p_k = 1, p_k \ge 0 \text{ for } k = 1, 2, \dots, n
$$</div>

<p><strong>Solution</strong></p>
<p>Applying the Lagrange multiplier method, we have</p>
<div>$$
L(p_1, p_2, \dots, p_n;\lambda) = -\sum_{k=1}^n p_k\ln p_k + \lambda\left(\sum_{k=1}^n p_k - 1\right)\\
\frac{\partial L}{\partial p_k} = -\ln p_k - 1 + \lambda = 0\\
\Rightarrow p_k = e^{\lambda - 1}\\
$$</div>

<p>Note that the above is true for all $k$. So we have</p>
<div>$$
p_k = e^{\lambda - 1}  = \frac1{n}\text{ for } k = 1, 2, \dots, n.
$$</div>

<p><strong>Continuous Case 1</strong></p>
<p>$X \in [-\infty, \infty]$.</p>
<p>Constrain on mean and variance,<br>we have the following optimization problem:</p>
<div>$$
\max_{X}h(X), \\
\text{s.t. }E[X] = \mu, \quad Var(X) = \sigma^2
$$</div>

<p>In detail, </p>
<div>$$
\max_{X} H(X) = \max_{\mu, \sigma^2} \left(-\int_{-\infty}^\infty f_X(x)\ln f_X(x)\mathrm dx\right)\\
\text{s.t. }\int_{-\infty}^\infty f(x)\mathrm dx = 1, \quad \int_{-\infty}^\infty xf(x)\mathrm dx = \mu, \quad \int_{-\infty}^\infty x^2f(x)\mathrm dx = \sigma^2 + \mu^2
$$</div>

<p>Solving the above problem, we have Gaussian distribution with mean $\mu$ and variance $\sigma^2$.</p>
<div>$$
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
$$</div>

<p><strong>Solution</strong></p>
<p>For all measurable functions $g$, we have</p>
<div>$$
G(t) = h(f + tg) = -\int_{\infty}^\infty (f(x) + tg(x))\ln (f(x) + tg(x))\mathrm dx
$$</div>

<p>Therefore,</p>
<div>$$
h(f_{opt})\ge h(f_{opt} + tg)\\
\Rightarrow G(0)\ge G(t), \forall t \in \R
$$</div>

<p>$G(t)$ reaches its maximum at $t &#x3D; 0$.</p>
<p>Then apply the Lagrange multiplier method, we have</p>
<div>$$
\overline{G}(t) = G(t) + c_0h_0(t) + c_1h_1(t) + c_2h_2(t)\\
$$</div>

<p>Get the derivative of $\overline{G}(t)$ with regard to $t$, and let the derivative equal to zero.</p>
<p><strong>Continuous Case 2</strong></p>
<p>$X \in [0, \infty)$.</p>
<p>Constrain on mean only, we have the following optimization problem:</p>
<div>$$
\max_{X}h(X), \\
\text{s.t. }E[X] = \mu
$$</div>

<p>In detail,</p>
<div>$$
\max_{X} H(X) = \max_{\mu} \left(-\int_{0}^\infty f_X(x)\ln f_X(x)\mathrm dx\right)\\
\text{s.t. }\int_{0}^\infty f(x)\mathrm dx = 1, \quad \int_{0}^\infty xf(x)\mathrm dx = \mu
$$</div>

<p>Solving the above problem, we have exponential distribution with parameter $\lambda$.</p>
<div>$$
f(x) = \lambda e^{-\lambda x}, x \in [0, \infty)
$$</div>

<p><strong>Continuous Case 3</strong></p>
<p>$X \in [a, b]$.</p>
<p>No constrain, we have the unconstrained optimization problem:</p>
<div>$$
\max_{X}h(X)
$$</div>

<p>In detail,</p>
<div>$$
\max_{X} H(X) = \max_{a, b} \left(-\int_{a}^b f_X(x)\ln f_X(x)\mathrm dx\right)\\
\text{s.t. }\int_{a}^b f(x)\mathrm dx = 1
$$</div>

<p>Solving the above problem, we have uniform distribution within $[a, b]$.</p>
<div>$$
f(x) = \frac{1}{b-a}, x \in [a, b]
$$</div>

<h2 id="Convolution-covariance-correlation-and-conditional-expectation"><a href="#Convolution-covariance-correlation-and-conditional-expectation" class="headerlink" title="Convolution, covariance, correlation, and conditional expectation"></a>Convolution, covariance, correlation, and conditional expectation</h2><h3 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h3><p><strong>Discrete case</strong></p>
<div>$$
\begin{align*}
p_W(w) &= P(X+Y=w)\\
&= \sum_{x}P(X=x, Y=w-x)\\
&= \sum_{x}P(X=x)P(Y=w-x)\\
&= \sum_{x}p_X(x)p_Y(w-x)\\
\end{align*}
$$</div>

<p>PMF $p_W$ is the convolution of PMFs $p_X$ and $p_Y$.</p>
<p><strong>The distribution of $X+Y$</strong></p>
<p>Mechanics:</p>
<ul>
<li>Put the PMF‚Äôs on top of each other</li>
<li>Flip the PMF of $Y$</li>
<li>Shift the flipped PMF by $w$ (to the right if $w&gt;0$)</li>
<li>Cross-multiply and add</li>
</ul>
<p><strong>Continuous Case</strong></p>
<div>$$
\begin{align*}
&W = X+Y, X, Y \text{ are independent}\\
&P(W\le w|X=x) = P(Y\le w-x)\\
&f_{W|X}(w|x) = f_Y(w-x)\\
&f_{W, X}(w, x) = f_X(x)f_Y(w-x)\\
&f_W(w) = \int_{-\infty}^\infty f_X(x)f_Y(w-x)\mathrm dx\\
\end{align*}
$$</div>

<p><strong>Sum of 2 independent normal RVs</strong></p>
<div>$$
\begin{align*}
    & X\sim N(\mu_1, \sigma_1^2), Y\sim N(\mu_2, \sigma_2^2)\\
    &f_{X,Y}(x, y) = \frac{1}{2\pi \sigma_x\sigma_y}\text{exp}\left\lbrace-\frac{(x-\mu_x)^2}{2\sigma_x^2} - \frac{(y-\mu_y)^2}{2\sigma_y^2}\right\rbrace
\end{align*}
$$</div>

<p>which is constant on the ellipse(circle if $\sigma_x &#x3D; \sigma_y$).</p>
<div>$$
\begin{align*}
    X\sim N(0, \sigma_x), &Y\sim N(0, \sigma_y)\\
    W &= X+Y\\
    f_W(w) &= \int_{-\infty}^\infty f_{X,Y}(x, w-x)\mathrm dx\\
    &= \int_{-\infty}^\infty \frac{1}{2\pi \sigma_x\sigma_y}\text{exp}\left\lbrace-\frac{x^2}{2\sigma_x^2} - \frac{(w-x)^2}{2\sigma_y^2}\right\rbrace\mathrm dx\\
    =ce^{-\gamma \omega^2}
\end{align*}
$$</div>

<p>$W$ is Normal.</p>
<p>Mean &#x3D; 0, Variance &#x3D; $\sigma_x^2 + \sigma_y^2$</p>
<p>Same argument for nonzero mean case.</p>
<p><strong>The difference of two independent RVs</strong></p>
<p>$X$ and $Y$  are independent exponential RVs with parameter $\lambda$.</p>
<p>Fix some $z\ge 0$ and note that $f_Y(x-z)$ is non zero when $x\ge z$.</p>
<div>$$
\begin{align*}
    Z &= X - Y\\
    f_Z(z) &= \int_{-\infty}^\infty f_X(x)f_{-Y}(z - x)\mathrm dx\\
    &= \int_{-\infty}^\infty f_X(x)f_{Y}(x - z)\mathrm dx\\
    &= \int_{z}^\infty \lambda e^{-\lambda x}\lambda e^{-\lambda(x-z)}\mathrm dx\\
    &= \frac{\lambda}{2}e^{-\lambda z}
\end{align*}
$$</div>

<p>The answer for the case $z\le 0$</p>
<div>$$
f_{X-Y}(z) = f_{Y-X}(z) = f_Z(-z)
$$</div>

<p>The first quality holds by symmetry.</p>
<h3 id="Covariance-and-Correlation"><a href="#Covariance-and-Correlation" class="headerlink" title="Covariance and Correlation"></a>Covariance and Correlation</h3><p><strong>Definition</strong></p>
<p>The covariance of two RVs $X$ and $Y$, denoted by $\text{cov}(X, Y)$, is defined by</p>
<div>$$
\text{cov}(X, Y) = E\left[(X - E[X])(Y - E[Y])\right]
$$</div>

<p>or, </p>
<div>$$
\text{cov}(X, Y) = E[XY] - E[X]E[Y]
$$</div>

<p>$X$ and $Y$ are <strong>uncorrelated</strong> if $\text{cov}(X, Y) &#x3D; 0$.</p>
<p><strong>Zero mean case</strong> $\text{cov}(X, Y) &#x3D; E[XY]$</p>
<p><strong>Properties</strong></p>
<div>$$
\text{cov}(X, Y) = \text{var}(X, Y)\\
\text{cov}(X, aY+b) = a\cdot\text{cov}(X, Y)\\
\text{cov}(X, Y+Z) = \text{cov}(X, Y) + \text{cov}(X, Z)\\
\text{independent} \Rightarrow \text{cov}(X, Y) = 0(\text{converse is not true})
$$</div>

<p><strong>Variance of the sum of RVs</strong></p>
<div>$$
\text{var}\left(\sum_{i = 1}^nX_i\right) = \sum_{i = 1}^n\text{var}(X_i) + \sum_{\lbrace(i, j)|i\ne j\rbrace}\text{cov}(X_i, X_j)
$$</div>

<p>In particular, </p>
<div>$$
\text{var}(X_1 + X_2) = \text{var}(X_1) + \text{var}(X_2) + 2\text{cov}(X_1, X_2)
$$</div>

<p><strong>Correlation coefficient</strong></p>
<p>The correlation coefficient $\rho(X, Y)$ of two RVs $X$ and $Y$ that have nonzero variance is defined as</p>
<div>$$
\begin{align*}
\rho &= E\left[\frac{(X - E[X])}{\sigma_X} \cdot \frac{(Y - E[Y])}{\sigma_Y}\right]\\
&= \frac{\text{cov}(X, Y)}{\sigma_X\sigma_Y}
\end{align*}
$$</div>

<ul>
<li>$-1 \le \rho \le 1$</li>
<li>$|\rho| &#x3D; 1 \Leftrightarrow (X-E[X]) &#x3D; c(Y-E[Y])$</li>
<li>Independent $\Rightarrow \rho &#x3D; 0(\text{converse is not true})$</li>
</ul>
<p><strong>Conditional expected value</strong></p>
<div>$$
E[X|Y = y] = \sum_x xp_{X|Y}(x|y)
$$</div>

<h3 id="Conditional-expectation"><a href="#Conditional-expectation" class="headerlink" title="Conditional expectation"></a>Conditional expectation</h3><p><strong>Definition</strong></p>
<div>$$
E[X|Y = y] = \begin{cases}
    \sum_x xp_{X|Y}(x|y), & X \text{discrete},\\
    \int_{-\infty}^\infty xf_{X|Y}(x|y)\mathrm dx, & X \text{continuous}.
\end{cases}
$$</div>

<p>$E[X|Y&#x3D;y]$ is a function of $y$.</p>
<div>$$
E[X|Y = y] = \frac{y}{2}(\text{number})\\
E[X|Y] = \frac{Y}{2}(\text{RV})
$$</div>

<p><strong>Law of iterated expectations</strong></p>
<div>$$
E[X] = E[E[X|Y]] = \begin{cases}
    \sum_y E[X | Y = y]p_Y(y), & Y \text{discrete},\\
    \int_{-\infty}^\infty E[X|Y = y]f_Y(y)\mathrm dy, & Y \text{continuous}.
\end{cases}
$$</div>

<h3 id="Conditional-expectation-as-an-estimator"><a href="#Conditional-expectation-as-an-estimator" class="headerlink" title="Conditional expectation as an estimator"></a>Conditional expectation as an estimator</h3><p>Denote the conditional expectation</p>
<div>$$
\hat{X} = E[X|Y]
$$</div>

<p>as an estimator of $X$ given $Y$, and the estimation error</p>
<div>$$
\tilde{X} = X - \hat{X}
$$</div>

<p>is a RV.</p>
<p><strong>Properties of the estimator</strong>: </p>
<p><strong>Unbiased</strong></p>
<p>For <strong>any</strong> possible $Y&#x3D;y$:</p>
<div>$$
E[\tilde{X}|Y] = E[X - \hat{X} | Y] = E[X | Y] - E[\hat{X}|Y] = \hat{X} - \hat{X} = 0
$$</div>

<p>By the law of iterated expectations</p>
<div>$$
E[\tilde{X}] = E[E[\tilde{X}|Y]] = 0
$$</div>

<p><strong>Uncorrelated</strong></p>
<div>$$
E[\hat{X}\tilde{X}] = E[E[\hat{X}\tilde{X}|Y]] = E[\hat{X}E[\tilde{X}|Y]] = 0
$$</div>

<div>$$
\text{cov}(\hat{X}, \tilde{X}) = E[\hat{X}\tilde{X}] - E[\hat{X}]E[\tilde{X}] = 0
$$</div>

<p>Since $X &#x3D; \hat{X} + \tilde{X}$, the variance of X can be decomposed as</p>
<div>$$
\text{var}(X) = \text{var}(\hat{X}) + \text{var}(\tilde{X})
$$</div>

<div>$$
\text{var}(\tilde{X}) = \text{var}(E[X|Y])
$$</div>

<p>Conditional variance</p>
<div>$$
\text{var}(X|Y) = E[(X - E[X|Y])^2|Y] = E[\tilde{X}^2|Y]
$$</div>

<p>here comes the law of total variance:</p>
<div>$$
\text{var}(X) = \text{var}(E[X|Y]) + E[\text{var}(X|Y)] 
$$</div>

<p>The total variability is avarage variability within sections + variability between sections.</p>
<p><strong>Law of iterated expectations</strong></p>
<div>$$
E[X] = E[E[X|Y]] = \sum_y E[X|Y = y]p_Y(y)
$$</div>


<p><strong>Conditional variance</strong></p>
<div>$$
\text{var}(X|Y) = E[(X - E[X|Y])^2|Y] = E[\tilde{X}^2|Y]
$$</div>

<p><strong>Law of total variance</strong></p>
<div>$$
\text{var}(X) = \text{var}(E[X|Y]) + E[\text{var}(X|Y)] 
$$</div>


<h2 id="Transforms-and-sum-of-a-random-number-of-random-variables"><a href="#Transforms-and-sum-of-a-random-number-of-random-variables" class="headerlink" title="Transforms and sum of a random number of random variables"></a>Transforms and sum of a random number of random variables</h2><p>The transform associated with a RV $X$ is a function $M_X(s)$ of a scalar parameter $s$, defined by</p>
<div>$$
M_X(s) = E[e^{sX}] = \begin{cases}
    \sum_x e^{sx}p_X(x), & X \text{discrete},\\
    \int_{-\infty}^\infty e^{sx}f_X(x)\mathrm dx, & X \text{continuous}.
\end{cases}
$$</div>

<p><strong>Remarks</strong></p>
<ul>
<li>a function of $s$, rather than a number</li>
<li>not necessarily defined for all (complex) s</li>
<li>always well defined for $\Re(s)&#x3D;0$</li>
<li>compared with Laplace transform</li>
</ul>
<h3 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h3><p><strong>Sanity Checks</strong></p>
<div>$$
M_X(0) = 1\\
|M_X(s)| \le 1 \text{ for } \Re(s) = 0
$$</div>

<p><strong>Linear operation</strong></p>
<div>$$
M_{aX + b}(s) = e^{bs}M_X(as)\\
M_{X + Y}(s) = M_X(s)M_Y(s) (\text{if X, Y independent})
$$</div>

<p><strong>Expected Values</strong></p>
<div>$$
E[X^n] = \frac{\partial^n M_X(s)}{\partial s^n}\bigg|_{s=0}
$$</div>

<div>$$
P(X = c) = \lim_{N\rightarrow \infty}\frac 1N\sum_{k = 1}^N M_X(jk)e^{-jkc}
$$</div>

<p>since</p>
<div>$$
\lim_{N\rightarrow \infty}\frac 1N\sum_{k = 1}^N M_X(jk)e^{-jkc} = \sum_{x = 1}^\infty p_X(x)\lim_{N\rightarrow \infty}\frac 1N\sum_{k = 1}^N e^{-jc(k - x)} = \sum_{x = 1}^\infty p_X(x)\lim_{N\rightarrow \infty}\frac{1}{N} \frac{e^{j(x-c)} - e^{Nj(x - c)}}{1-e^{j(x-c)}} = p_X(c)
$$</div>

<p><strong>Example</strong></p>
<p>$X$ is a Poisson RV with parameter $\lambda$</p>
<div>$$
p_X(x) = \frac{\lambda^x}{x!}e^{-\lambda}
$$</div>

<div>$$
M(s) = \sum_{x = 0}^\infty e^{sx}\frac{\lambda^x}{x!}e^{-\lambda} = e^{-\lambda}\sum_{x = 0}^\infty \frac{(e^s\lambda)^x}{x!} = e^{-\lambda}e^{e^s\lambda} = e^{\lambda(e^s - 1)}
$$</div>

<p>$X$ is an exponential RV with parameter $\lambda$</p>
<div>$$
f_X(x) = \lambda e^{-\lambda x}
$$</div>

<div>$$
M(s) = \int_0^\infty e^{sx}\lambda e^{-\lambda x}\mathrm dx = \frac{\lambda}{\lambda - s}
$$</div>

<p>$Y$ is a standard normal RV, </p>
<div>$$
M_Y(s) = \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}e^{-(y^2/2)}e^{sy}\mathrm dy = e^{s^2/2}
$$</div>

<p>Consider $X &#x3D; \sigma Y + \mu$</p>
<div>$$
M_X(s) = e^{s^2\sigma^2/2 + \mu s}
$$</div>

<h3 id="Inversion-of-transforms"><a href="#Inversion-of-transforms" class="headerlink" title="Inversion of transforms"></a>Inversion of transforms</h3><p><strong>Inversion Property</strong></p>
<p>The transform $M_X(s)$ associated with a RV $X$ uniquely determines the CDF of $X$, assuming that $M_X(s)$ is finite for all $s$ in some interval $[-a, a]$, where $a$ is a positive number.</p>
<p>Example:</p>
<div>$$
\begin{align*}
M(s) &= \frac{pe^s}{1 - (1 - p)e^s}\\
&= pe^s(1 + (1-p)e^s + (1-p)^2e^{2s} + \dotsb)\\
&= \sum_{k = 1}^\infty p(1-p)^{k - 1}e^{ks}
\end{align*}
$$</div>

<p>The probability $P(X &#x3D; k)$ is found by reading the coefficient of the term $e^{ks}$:</p>
<div>$$
P(X = k) = p(1-p)^{k-1}
$$</div>

<h3 id="Transform-of-Mixture-of-Distributions"><a href="#Transform-of-Mixture-of-Distributions" class="headerlink" title="Transform of Mixture of Distributions"></a>Transform of Mixture of Distributions</h3><p>Let $X_1,\dotsb, X_n$ be continuous RVs with PDFs $f_{X_1}, \dotsb, f_{X_n}$.</p>
<p>The value $y$ of RV $Y$ is generated as follows: an index $i$ is chosen with a corresponding probability $p_i$, and $y$ is taken to be equal to the value $X_i$. Then, </p>
<div>$$
f_Y(y) = p_1f_{X_1}(y) + \dotsb + p_nf_{X_n}(y)\\
M_Y(s) = p_1M_{X_1}(s) + \dotsb + p_nM_{X_n}(s)
$$</div>

<h3 id="Sum-of-independend-RVs"><a href="#Sum-of-independend-RVs" class="headerlink" title="Sum of independend RVs"></a>Sum of independend RVs</h3><p>Let $X$ and $Y$ be independent RVs, and let $Z &#x3D; X + Y$. The transform associated with $Z$ is </p>
<div>$$
M_Z(s) = M_X(s)M_Y(s)
$$</div>

<p>Since</p>
<div>$$
M_Z(s) = E[e^{sZ}] = E[e^{s(X + Y)}] = E[e^{sX}e^{sY}] = E[e^{sX}]E[e^{sY}] = M_X(s)M_Y(s)
$$</div>

<p>Generalization:</p>
<p>A collection of independent RVs: $X_1, \dotsb, X_n$, $Z &#x3D; X_1 + \dotsb + X_n$ ,</p>
<div>$$
M_Z(s) = M_{X_1}(s)\dotsb M_{X_n}(s)
$$</div>

<p><strong>Example</strong></p>
<p>Let $X_1, \dotsb, X_n$ be independent Bernoulli RVs with a common parameter $p$:</p>
<div>$$
M_{X_i}(s) = 1 - p + pe^s
$$</div>

<p>$Z &#x3D; X_1 + \dotsb + X_n$ is binomial with parameters n and p:</p>
<div>$$
M_z(s) = (1 - p + pe^s)^n
$$</div>

<p>Let $X$ and $Y$ be independent Poisson RVs with means $\lambda$ and $\mu$, and let $Z &#x3D; X + Y$. Then $Z$ is still Poisson with mean $\lambda + \mu$.</p>
<div>$$
M_Z(s) = M_X(s)M_Y(s) = e^{(\lambda +\mu)(e^s - 1)}
$$</div>

<p>Let $X$ and $Y$ be independent Gaussian RVs with means $\mu_x$ and $\mu_y$, and variances $\sigma_x^2, \sigma_y^2$. And let $Z &#x3D; X + Y$. Then $Z$ is still Gaussian with mean $\mu_x + \mu_y$ and variance $\sigma_x^2 + \sigma_y^2$</p>
<div>$$
M_X(s) = \exp\bigg\lbrace\frac{\sigma_x^2s^2}{2} + \mu_x s\bigg\rbrace\\
M_Y(s) = \exp\bigg\lbrace\frac{\sigma_y^2s^2}{2} + \mu_y s\bigg\rbrace\\
M_Z(s) = M_X(s)M_Y(s) = \exp\bigg\lbrace\frac{(\sigma_x^2 + \sigma_y^2)s^2}{2} + (\mu_x + \mu_y)s\bigg\rbrace
$$</div>

<p>Consider</p>
<div>$$
Y = X_1 + \dotsb + X_N
$$</div>

<p>where $N$ is a RV that takes integer values, and $X_1, \dotsb, X_N$ are identically distributed RVs.</p>
<p>Assume that $N, X_1, \dotsb$ are independent.</p>
<div>$$
E[Y|N = n] = E[X_1 + X_2 + \dotsb + X_n|N = n] = nE[X]\\
E[Y|N] = NE[X]\\
E[Y] = E[E[Y|N]] = E[NE[X]] = E[N]E[X]
$$</div>

<p>For the variance, </p>
<div>$$
E[Y|N] = NE[X]\\
\text{var}(E[Y|N]) = (E[X])^2\text{var}(N)\\
\text{var}(Y|N=n) = n\text{var}(X)\\
\text{var}(Y|N) = N \text{var}(X)\\
E[\text{var}(Y|N)] = E[N]\text{var}(X)\\
$$</div>

<p>So, </p>
<div>$$
\text{var}(Y) = E[\text{var}(Y|N)] + \text{var}(E[Y|N]) = E[N]\text{var}(X) + (E[X])^2\text{var}(N)
$$</div>

<p>For transform,</p>
<div>$$
E[e^{sY}|N = n] = E[e^{sX_1}\dotsb e^{sX_n}|N = n] = E[e^{sX}]^n = (M_X(s))^n\\
M_Y(s) = E[e^{sY}] = E[E[e^{sY}|N]] = E[(M_X(s))^N] = \sum_{n = 0}^\infty (M_X(s))^n p_N(n) = \sum_{n = 0}^\infty e^{n\log M_X(s)}p_N(n) = M_N(\log M_X(s))
$$</div>

<p><strong>Summary on Properties</strong></p>
<p>Consider the sum</p>
<div>$$
Y = X_1 + \dotsb + X_N
$$</div>

<p>where $N$ is a RV that takes integer values, and $X_1, X_2, \dotsb$ are identically distributed RVs. Assume that $N$, $X_1, X_2, \dotsb$ are independent.</p>
<div>$$
E[Y] = E[N]E[X]\\
\text{var}(Y) = E[N]\text{var}(X) + (E[X])^2\text{var}(N)\\
M_Y(s) = M_N(\log M_X(s))
$$</div>

<p><strong>Example</strong></p>
<p>Assume that $N$ and $X_i$ are both geometrically distributed with parameters $p$ and $q$ respectively. All of these RVs are independent. $Y &#x3D; X_1 + \dotsb + X_N$</p>
<div>$$
M_N(s) = \frac{pe^s}{1 - (1-p)e^s}\\
M_X(s) = \frac{qe^s}{1 - (1-q)e^s}\\
M_Y(s) = M_N(\log M_X(s)) = \frac{pqe^s}{1 - (1-pq)e^s}
$$</div>

<p>$Y$ is also geometrically distributed, with parameter $pq$.</p>
<h2 id="Weak-law-of-large-numbers"><a href="#Weak-law-of-large-numbers" class="headerlink" title="Weak law of large numbers"></a>Weak law of large numbers</h2><h3 id="Markov-inequality"><a href="#Markov-inequality" class="headerlink" title="Markov inequality"></a>Markov inequality</h3><p>If a RV $X$ can only take nonnegative values, then</p>
<div>$$
P(X \ge a) \le \frac{E[X]}{a}, \text{ for all } a \gt 0.
$$</div>

<p>Intuition: If a nonnegative RV has a small mean, then the probability that it takes a large value must be small„ÄÇ</p>
<p>Fix a positive number $a$, </p>
<div>$$
E[X] = \int_0^\infty xf_X(x)dx = \int_0^a xf_X(x)dx + \int_a^\infty xf_X(x)dx \ge 0 + \int_a^\infty xf(x)dx \ge \int_a^\infty af_X(x)dx = aP(X \ge a)
$$</div>

<h3 id="Chebyshev‚Äôs-Inequality"><a href="#Chebyshev‚Äôs-Inequality" class="headerlink" title="Chebyshev‚Äôs Inequality"></a>Chebyshev‚Äôs Inequality</h3><p>If $X$ is a RV with mean $\mu$ and variance $\sigma^2$, then</p>
<div>$$
P(|X - \mu| \ge c) \le \frac{\sigma^2}{c^2}
$$</div>

<p>Intuition: If a RV has small variance, then the probability that it takes a value far from its mean is also small.</p>
<div>$$
\begin{align*}
\sigma^2 &= \int (x - \mu)^2f_X(x)\mathrm dx\\
&\ge \int_{-\infty}^{\mu - c} (x - \mu)^2f_X(x)\mathrm dx + \int_{ \mu + c}^\infty (x - \mu)^2f_X(x)\mathrm dx\\
&\ge \int_{-\infty}^{\mu - c} c^2f_X(x)\mathrm dx + \int_{ \mu + c}^\infty c^2f_X(x)\mathrm dx\\
&= \int_{|x - \mu| \ge c} c^2f_X(x)\mathrm dx\\
&=c^2P(|X - \mu| \ge c)
\end{align*}
$$</div>

<p>The upperbounds of $\sigma^2$:</p>
<div>$$
X \in [a, b]\\
\sigma^2 \le (b - a)^2/4
$$</div>

<p><strong>Chernoff inequality</strong></p>
<p>If a RV $X$ has MGF $M_X(s)$, then</p>
<div>$$
P(X \ge a) \le e^{-\max_{s\ge 0}\left(sa - \ln M_X(s)\right)}
$$</div>

<p>or, for $s \ge 0$</p>
<div>$$
P(X\ge a) \le e^{-sa}M_X(s)
$$</div>

<p>for $s \lt 0$</p>
<div>$$
P(X \le a) \le e^{-sa}M_X(s)
$$</div>

<p>proof: for $s \ge 0$</p>
<div>$$
M_X(s) = \int_{-\infty}^a e^{sx}f_X(x)\mathrm dx + \int_a^{\infty} e^{sx}f_X(x)\mathrm dx\\
\ge 0 + e^{sa}\int_a^{\infty} f_X(x)\mathrm dx = e^{sa}P(X \ge a)
$$</div>

<h3 id="Weak-law-of-large-numbers-1"><a href="#Weak-law-of-large-numbers-1" class="headerlink" title="Weak law of large numbers"></a>Weak law of large numbers</h3><p>Let $X_1, X_2, \dots$ be independent identically distributed (i.i.d.) RVs with finite mean $\mu$ and variance $\sigma^2$</p>
<div>$$
M_n = \frac{X_1 + X_2 + \dotsb + X_n}{n}\\
E[M_n] = \mu\\
\text{var}(M_n) = \frac{\sigma^2}{n}
$$</div>

<p>Applying the Chebyshev inequality and we get:</p>
<div>$$
P(|M_n - \mu| \ge \epsilon) \le \frac{\text{var}(M_n)}{\epsilon^2} = \frac{\sigma^2}{n\epsilon^2}
$$</div>

<p>For large $n$, the bulk of the distribution of $M_n$ is concentrated near $\mu$</p>
<p><strong>Theorem</strong></p>
<p>Let $X_1, X_2, \dots$ be independent identically distributed (i.i.d.) RVs with finite mean $\mu$ and variance $\sigma^2$. For every $\epsilon \gt 0$, we have</p>
<div>$$
P(|M_n - \mu| \ge \epsilon) = P\left(\left|\frac{X_1 + \dotsb + X_n}{n} - \mu\right|\ge \epsilon\right) \rightarrow 0, \text{ as } n \rightarrow \infty
$$</div>

<p>$M_n$ converges <strong>in probability</strong> to $\mu$.</p>
<h3 id="Convergence-‚Äúin-Probability‚Äù"><a href="#Convergence-‚Äúin-Probability‚Äù" class="headerlink" title="Convergence ‚Äúin Probability‚Äù"></a>Convergence ‚Äúin Probability‚Äù</h3><p>Theorem: Convergence in Probability</p>
<p>Let $\lbrace Y_n\rbrace$(or $Y_1, Y_2, \dots$) be a sequence of RVs(not necessarily independent), and let $a$ be a real number. We say that the sequence $Y_n$ <strong>converges to</strong> $a$ in probability, if for every $\epsilon \gt 0$, we have </p>
<div>$$
\lim_{n \rightarrow \infty} P(|Y_n - a| \ge \epsilon) = 0
$$</div>

<p>(almost all) of the PMF&#x2F;PDF of $Y_n$, eventually gets concentrated (arbitrarily) close to $a$.</p>
<h3 id="Many-types-of-convergence"><a href="#Many-types-of-convergence" class="headerlink" title="Many types of convergence"></a>Many types of convergence</h3><p>Deterministic limits: $\lim_{n\rightarrow \infty} a_n &#x3D; a$</p>
<div>$$
|a_n - a|\le \epsilon, \forall n \ge N, \epsilon \gt 0
$$</div>

<p>Convergence in probability: $X_n\stackrel P{\rightarrow} X$</p>
<div>$$
\lim_{n \rightarrow \infty}P(|X_n - X|\ge \epsilon) = 0, \forall \epsilon \gt 0
$$</div>

<p>(WLLN)</p>
<p>Convergence in Distribution: $X_n \stackrel{D}{\rightarrow} X$</p>
<div>$$
\lim_{n \rightarrow \infty} P(X_n \le x) = P(X \le x), \forall x
$$</div>

<p>For all points of $x$ at which the function $F_X(x) &#x3D; P(X\le x)$is continuous.</p>
<p>(CLT)</p>
<p>Convergence with probability $1$(almost surely): $X_n \stackrel{\text{a.s.}}{\rightarrow} X$</p>
<div>$$
P\left(\lbrace\omega\in \Omega: \lim_{n\rightarrow\infty}X_n(\omega) =X(\omega)\rbrace\right) = 1
$$</div>

<p>or </p>
<div>$$
P\left(\lim_{n\rightarrow\infty}X_n(\omega) =X(\omega)\right) = 1
$$</div>

<p>Lemma:</p>
<div>$$
X_n \stackrel{\text{a.s.}}{\rightarrow} X \Leftrightarrow \lim_{m \rightarrow\infty}P(|X_n - X|\le \epsilon, \forall n \gt m) = 1, \forall \epsilon \gt 0\\
\Leftrightarrow P(|X_n - X|\gt \epsilon, \text{i.o.}) = 0, \forall \epsilon \gt 0
$$</div>

<p>i.o. stand for infinitely often</p>
<p>(SLLN)</p>
<p>Convergence in Mean&#x2F;in Norm: $X_n \stackrel{r}{\rightarrow}X$</p>
<p>if $E[X_n^r] \lt \infty$ for all $n$ and </p>
<div>$$
\lim_{n \rightarrow \infty}E[|X_n - X|^r] = 0
$$</div>

<p>Relations:</p>
<div>$$
\left(X_n\stackrel {\text{a.s.}}{\rightarrow} X\right) \Rightarrow\left(X_n\stackrel P{\rightarrow} X\right) \Rightarrow \left(X_n\stackrel D{\rightarrow} X\right) \\
\left(X_n\stackrel {r}{\rightarrow} X\right) \Rightarrow\left(X_n\stackrel P{\rightarrow} X\right) \Rightarrow \left(X_n\stackrel D{\rightarrow} X\right) \\
\forall r\ge s\ge 1, \left(X_n\stackrel {r}{\rightarrow} X\right) \Rightarrow\left(X_n\stackrel s{\rightarrow} X\right)
$$</div>

<p>The converse assertions fail in general!</p>
<p>The relation between ‚Äúalmost surely‚Äù and ‚Äúin r-th mean‚Äù is complicated. There exist sequences which converge almost surely but<br>not in mean, and which converge in mean but not almost surely!</p>
<h2 id="Central-Limit-Theorem"><a href="#Central-Limit-Theorem" class="headerlink" title="Central Limit Theorem"></a>Central Limit Theorem</h2><h3 id="Theorem"><a href="#Theorem" class="headerlink" title="Theorem"></a>Theorem</h3><p>Let $X_1, X_2, \dots$ be i.i.d. RVs with mean $\mu$ and variance $\sigma^2$. Let </p>
<div>$$
Z_n = \frac{X_1 + X_2 + \dotsb + X_n - n\mu}{\sigma\sqrt{n}}
$$</div>

<p>Then</p>
<div>$$
\lim_{n\rightarrow \infty}P(Z_n\le z) = \Phi (z) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^z e^{-\frac{x^2}{2}}\mathrm dx
$$</div>

<p>CDF of $Z_n$ converges to normal CDF(converge in distribution)</p>
<h3 id="Normal-Approximation-Based-on-the-Central-Limit-Theorem"><a href="#Normal-Approximation-Based-on-the-Central-Limit-Theorem" class="headerlink" title="Normal Approximation Based on the Central Limit Theorem"></a>Normal Approximation Based on the Central Limit Theorem</h3><p>Let $S_n &#x3D; X_1 + \dotsb + X_n$, where $X_i$ are $\text{i.i.d.}$ RVs with mean $\mu$ and variance $\sigma^2$. If $n$ is large, the probability $P(S_n ‚â§ c)$ can be approximated by<br>treating $S_n$ as if it were normal, according to the following procedure.</p>
<ol>
<li>Calculate the mean $n\mu$ and the variance $n\sigma^2$ of $S_n$</li>
<li>Calculate the normalinzd value $z &#x3D; (c - n\mu)&#x2F;(\sigma\sqrt{n})$</li>
<li>Use the approxmation</li>
</ol>
<div>$$
    P(S_n \le c)  \approx \Phi(z)
$$</div>

<p>where $\Phi(z)$ is available from the standard normal CDF.</p>
<h3 id="Proof"><a href="#Proof" class="headerlink" title="Proof"></a>Proof</h3><p>Suppose that $X_1, X_2, \dots$ has mean zero.</p>
<div>$$
\begin{align*}
M_{Z_n}(s) &= E[e^{sZ_n}]\\
&=E\left[\exp\left(\frac{s}{\sigma\sqrt{n}}\sum_{i = 1}^n X_i\right)\right]\\
&=\prod_{i = 1}^n E[e^{\frac{s}{\sigma\sqrt{n}}X_i}]\\
&=\prod_{i = 1}^n M_{X_i}\left(\frac{s}{\sigma\sqrt{n}}\right)\\
&=\left(M_{X}\left(\frac{s}{\sigma\sqrt{n}}\right)\right)^n\\
\end{align*}
$$</div>

<p>Suppose that the transform $M_X(s)$ has a second order Taylor series expansion around $s&#x3D;0$,</p>
<div>$$
M_X(s) = a + bs + cs^2 + o(s^2)
$$</div>

<p>where $a &#x3D; M_X(0) &#x3D; 1, b &#x3D; M_X‚Äô(0) &#x3D; E[X] &#x3D; 0, c &#x3D; \frac{1}{2}M_X‚Äô‚Äô(0) &#x3D; \frac{\sigma^2}{2}$</p>
<p>Then</p>
<div>$$
M_{Z_n}(s) = \left(1 + \frac{s^2}{2n} + o\left(\frac{s^2}{n}\right)\right)^n
$$</div>

<p>As $n\rightarrow \infty$, </p>
<div>$$
\lim_{n\rightarrow \infty}M_{Z_n}(s) = \lim_{n\rightarrow \infty}\left(1 + \frac{s^2}{2n} + o\left(\frac{s^2}{n}\right)\right)^n = e^{\frac{s^2}{2}}
$$</div>

<p>Approxmation on binomial:</p>
<p>(De Moivre-Laplace Approxmation to the Binomial)</p>
<div>$$
P(k \le S_n \le l) = P\left(\frac{k - np}{\sqrt{np(1-p)}} \le \frac{S_n - np}{\sqrt{np(1 - p)}} \le \frac{l - np}{\sqrt{np(1 - p)}}\right)\\
\approx \Phi\left(\frac{l - np}{\sqrt{np(1 - p)}}\right) - \Phi\left(\frac{k - np}{\sqrt{np(1 - p)}}\right)
$$</div>

<h2 id="The-Strong-Law-of-Large-Numbers"><a href="#The-Strong-Law-of-Large-Numbers" class="headerlink" title="The Strong Law of Large Numbers"></a>The Strong Law of Large Numbers</h2><h3 id="Theorem-1"><a href="#Theorem-1" class="headerlink" title="Theorem"></a>Theorem</h3><p>Let $X_1, X_2, \dots$ be i.i.d. RVs with mean $\mu$.</p>
<div>$$
P(\lim_{n \rightarrow\infty}\frac{X_1 + \dots + X_n}{n} = \mu) = 1.
$$</div>

<h2 id="Borel-Cantelli-lemma-amp-Bernoulli-Process"><a href="#Borel-Cantelli-lemma-amp-Bernoulli-Process" class="headerlink" title="Borel-Cantelli lemma &amp; Bernoulli Process"></a>Borel-Cantelli lemma &amp; Bernoulli Process</h2><h3 id="Limit-of-set-sequence"><a href="#Limit-of-set-sequence" class="headerlink" title="Limit of set sequence"></a>Limit of set sequence</h3><div>$$
\limsup_n A_n = \bigcap_{n = 1}^\infty \bigcup_{k = n}^\infty A_k\\
\liminf_n A_n = \bigcup_{n = 1}^\infty \bigcap_{k = n}^\infty A_k
$$</div>

<p>If upper limit equals to lower limit, the limit of set sequence exists.</p>
<div>$$
\limsup_n A_n \supseteq \liminf_n A_n\\
\limsup_n A_n = \liminf_n A_n = \lim_n A_n
$$</div>

<p>Upper limit can also be denoted as</p>
<div>$$
\limsup_n A_n = \lbrace \omega: \omega \in A_n, \text{i.o.}\rbrace  = \lbrace A_n, \text{i.o.}\rbrace
$$</div>

<h3 id="Borel-Cantelli-Lemma"><a href="#Borel-Cantelli-Lemma" class="headerlink" title="Borel-Cantelli Lemma"></a>Borel-Cantelli Lemma</h3><p>Let $\lbrace A_n, n &#x3D; 1, 2, \dotsb\rbrace$ be a sequence of events, then</p>
<div>$$
\sum_{n = 1}^\infty P(A_n)\lt \infty \xRightarrow{} P(A_n, \text{i.o.}) = 0
$$</div>

<p>Let $\lbrace A_n, n &#x3D; 1, 2, \dotsb\rbrace$ be a sequence of <strong>independent</strong> events, then</p>
<div>$$
\sum_{n = 1}^\infty P(A_n) = \infty \xRightarrow{} P(A_n, \text{i.o.}) = 1
$$</div>

<h3 id="Stochastic-process"><a href="#Stochastic-process" class="headerlink" title="Stochastic process"></a>Stochastic process</h3><p>A stochastic process is a mathematical model of a probabilistic experiment that evolves in time and generates a sequence of<br>numerical values.</p>
<ul>
<li>Bernoulli process(memoryless, discrete time)</li>
<li>Poisson process(memoryless, continuous time)</li>
</ul>
<h3 id="The-Bernoulli-Process"><a href="#The-Bernoulli-Process" class="headerlink" title="The Bernoulli Process"></a><strong>The Bernoulli Process</strong></h3><p>is a sequence of independent Bernoulli trials, each with probability of success $p$.</p>
<div>$$
P(\text{success}) = P(X_i = 1) = p\\
P(\text{failure}) = P(X_i = 0) = 1 - p
$$</div>

<p><strong>Independence property</strong>: For any given time $n$, the sequence of $X_{n + 1}, X_{n + 2}, \dots$ is also a Bernoulli process, and is independent from $X_1, \dots, X_n$</p>
<p><strong>Memoryless property</strong>: Let $n$ be a given time and let $\overline T$ be the time of the first success after<br>time $n$. Then $\overline T ‚àí n$ has a geometric distribution with parameter $p$,<br>and is independent of the RVs $X_1, \dots , X_n$.</p>
<div>$$
P(\overline T - n = t | \overline T \gt n) = (1 - p)^{t - 1}p = P(T = t)
$$</div>

<p><strong>Interarrival times</strong></p>
<p>Denote the $k$th success as $Y_k$, the $k$th interarrival time as $T_k$.</p>
<div>$$
T_1 = Y_1, T_k = Y_k - Y_{k - 1}, k = 2, 3, \dots
$$</div>

<p>represents the number of trials following the $(k - 1)$th success until the next success.</p>
<p>Note that</p>
<div>$$
Y_k = T_1 + T_2 + \dotsb + T_k
$$</div>

<p>Alternative description of the Bernoulli process: </p>
<ul>
<li>Start with a sequence of independent geometric RVs $T_1, T_2, \dots$ with common parameter p, and let these stand for the interarrival times.</li>
<li>Record a success at times $T_1$, $T_1 + T_2$, etc.</li>
</ul>
<div>$$
E[Y_k] = \frac{k}{p}\\
\text{var}(Y_k) = \frac{k(1 - p)}{p^2}
$$</div>

<div>$$
p_{Y_k}(t) = \binom{t - 1}{k - 1}p^k(1 - p)^{t - k}
$$</div>

<p><strong>Splitting of a Bernoulli process</strong></p>
<p>Whenever there is an arrival, we choose to either keep it (with probability $q$), or to discard it (with probability $1 ‚àí q$).</p>
<p>Both the process of arrivals that are kept and the process of discarded arrivals are Bernoulli processes, with success probability $pq$ and $p(1 ‚àí q)$, respectively, at each time.</p>
<p><strong>Merging of a Bernoulli process</strong></p>
<p>In a reverse situation, we start with two independent Bernoulli processes (with parameters $p$ and $q$ respectively). An arrival is<br>recorded in the merged process if and only if there is an arrival in at least one of the two original processes.</p>
<p>The merged process is Bernoulli, with success probability $p+q‚àípq$ at each time step.</p>
<h2 id="The-Poisson-Process"><a href="#The-Poisson-Process" class="headerlink" title="The Poisson Process"></a>The Poisson Process</h2><h3 id="Definition-1"><a href="#Definition-1" class="headerlink" title="Definition"></a>Definition</h3><p>An arrival process is called a Poisson process with rate $Œª$ if it has the following properties:</p>
<p><strong>Time homogenity</strong></p>
<div>$$
P(k, \tau) = P(k \text{ arrivals in interval of duration }\tau)
$$</div>

<p><strong>Independence</strong></p>
<p>Numbers of arrivals in disjoint time intervals are independent.</p>
<p><strong>Small interval probabilities</strong></p>
<div>$$
\begin{cases}
    1 - \lambda\tau + o(\tau), & \text{if } k = 0,\\
    \lambda\tau + o_1(\tau), & \text{if } k = 1,\\
    o_k(\tau), & \text{if } k > 1.
\end{cases}
$$</div>

<h3 id="Bernoulli-x2F-Poisson-Relation"><a href="#Bernoulli-x2F-Poisson-Relation" class="headerlink" title="Bernoulli&#x2F;Poisson Relation"></a>Bernoulli&#x2F;Poisson Relation</h3><p>In a short time interval $\delta$</p>
<div>$$
n = t / \delta\\
p = \lambda\delta\\
np = \lambda t
$$</div>

<p>For binomial PMF $p_S(k;n,p)$,</p>
<div>$$
\lim_{n\rightarrow \infty}p_S(k;n, p) = \lim_{n\rightarrow\infty}\frac{n!}{(n - k)!k!}p^k(1 - p)^{n - k} = \frac{(\lambda t)^k}{k!}e^{-\lambda t} = P(k, t)
$$</div>

<h3 id="PMF-of-Number-of-Arrivals-N"><a href="#PMF-of-Number-of-Arrivals-N" class="headerlink" title="PMF of Number of Arrivals $N$"></a>PMF of Number of Arrivals $N$</h3><div>$$
P(k, \tau) = \frac{(\lambda\tau)^ke^{-\lambda\tau}}{k!}
$$</div>

<div>$$
E[N_t] = \lambda t\\
\text{var}(N_t) = \lambda t
$$</div>

<h3 id="Time-T-of-the-first-arrival"><a href="#Time-T-of-the-first-arrival" class="headerlink" title="Time $T$ of the first arrival"></a>Time $T$ of the first arrival</h3><div>$$
F_T(t) = P(T \le t) = 1 - P(T \gt t) = 1 - e^{-\lambda t}, t\ge 0\\
f_T(t) = \lambda e^{-\lambda t}, t\ge 0
$$</div>

<p><strong>Memoryless property</strong> The  time to next arrival is independent of the past.</p>
<h3 id="Interarrival-times"><a href="#Interarrival-times" class="headerlink" title="Interarrival times"></a>Interarrival times</h3><p>We also denote the time of the kth success as $Y_k$, and denote the<br>kth interarrival time as $T_k$. That is,</p>
<div>$$
T_1 = Y_1, T_k = Y_k - Y_{k - 1}, k = 2, 3, \dots
$$</div>

<p>Note that</p>
<div>$$
Y_k = T_1 + T_2 + \dotsb + T_k
$$</div>

<div>$$
f_{Y_k}(y) = \frac{\lambda^ky^{k-1}e^{-\lambda y}}{(k - 1)!}, y\ge 0
$$</div>

<h3 id="Merging-Poisson-Processes"><a href="#Merging-Poisson-Processes" class="headerlink" title="Merging Poisson Processes"></a>Merging Poisson Processes</h3><p><img src="/../images/prob/L14_1.jpg" loading="lazy"></p>
<div>$$
P(\text{Arrival is red} | \text{1 arrival})\approx \frac{\lambda_1\delta}{(\lambda_1 + \lambda_2) \delta}
$$</div>

<p><img src="/../images/prob/L15_1.jpg" loading="lazy"></p>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>Post author: </strong>Blither Boom</li><li class="post-copyright-link"><strong>Post link: </strong><a href="https://blitherboom812.github.io/2023/02/20/Introduction-to-Probability/" title="Introduction-to-Probability">https://blitherboom812.github.io/2023/02/20/Introduction-to-Probability/</a></li><li class="post-copyright-license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> unless otherwise stated.</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/2023/02/22/Signal-and-Systems/" rel="prev" title="Signals and Systems"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">Signals and Systems</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/2023/01/21/CSAPP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="next" title="CSAPPÂ≠¶‰π†Á¨îËÆ∞"><span class="post-nav-text">CSAPPÂ≠¶‰π†Á¨îËÆ∞</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 ‚Äì 2025 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> Blither Boom</span></div><div class="powered"><span>Powered by <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> v6.2.0</span><span class="footer-separator">|</span><span>Theme - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.11</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a><script>function initMourn() {
  const date = new Date();
  const today = (date.getMonth() + 1) + "-" + date.getDate()
  const mourn_days = ["4-4","9-18","12-13"]
  if (mourn_days.includes(today)) {
    document.documentElement.style.filter = "grayscale(1)";
  }
}
initMourn();</script></body></html>