---
title: 通网
date: 2023-09-18 11:14:16
tags: note
katex: true
---

## 信息论基础

### 离散随机变量的信息度量

$$
H(X) = \mathbf E\{H(X=x_i)\} = -\sum_i p_i \log p_i
$$

称为熵

单位：
* 2 (Bit)
* e (Nat)
* 10 (Hartely)

表示了信息描述的有效性极限

信源编码（Source Coding），通过信息的有效表示，提高通信的有效性。例如: Huffman 编码

离散随机变量的最大熵：$\max_{p_i} H(X) = \log|S|$