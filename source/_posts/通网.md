---
title: 通网
date: 2023-09-18 11:14:16
tags: note
katex: true
---

## 信息论基础

### 离散随机变量的信息度量

$$
H(X) = \mathbf E\{H(X=x_i)\} = -\sum_i p_i \log p_i
$$

称为熵

单位：
* 2 (Bit)
* e (Nat)
* 10 (Hartely)

表示了信息描述的有效性极限

信源编码（Source Coding），通过信息的有效表示，提高通信的有效性。例如: Huffman 编码

离散随机变量的最大熵：$\max_{p_i} H(X) = \log|S|$

前缀码：任何码字都不是其他码字的前缀。前缀码保证了唯一可译码。是二叉树叶子节点。

**Kraft不等式**

对于信源字符集$\lbrace a_1, \dots, a_m\rbrace$，必满足：

$$
\sum\limits_{k=1}^{M}2^{-l(a_k)} \le 1
$$

同时，若上式成立，必存在码长分别为$𝑙(𝑎_𝑘)$的前缀码。


最小前缀码的平均码长：

$$
\min \bar L =\sum\limits_{i=1}^{M}p_il_i\\
s.t.\sum\limits_{i=1}^{M} 2^{-l_i} = 1
$$

由拉格朗日乘子法

$$
p_i = 2^{-l_i}, \bar L_{min} = -\sum\limits_{i=1}^{M}p_i \log p_i
$$

记 $H(X) = -\sum p_i\log p_i$ .一般的，上下界为$H(X) \le \bar L  \lt H(X) + 1$.

如果我们将$k$个独立同分布的信源符号 $x_1, \dots, x_k$堪称一个，对整体应用前缀码编码：

$$
\begin{align*}
    H(X_1, \dots, X_k) &= -\sum P(x_1, \dots, x_k) \log P(x_1, \dots, x_k)\\
    &= -\sum P(x_1, \dots, x_k) [\log P(x_1) +  \dots +  \log(x_k)]\\
    &= -\sum P(x_1)\log (x_1) - \dots - \sum P(x_k)\log (x_k)\\
    &= -kH(X)
\end{align*}
$$

直观：对长度为$𝑛$的$M$种信源符号序列，$𝑥_𝑖$出现的次数$≈𝑛𝑝_𝑖$

典型序列应满足上述分布，否则就“小众”“非典型”

典型的个数 # $≈ \frac{𝒏!}{(𝑛𝑝_1) !… (𝑛𝑝_M) !}$

平均每个信源符号可以用 $L = \frac{1}{n}\log\left(\frac{𝒏!}{(𝑛𝑝_1) !… (𝑛𝑝_M) !}\right)$ 个 bit 来表达。

通过 Stirling 公式可以得出 L的上下界：

$$

$$

故

$$
\lim\limits_{n\rightarrow \infty}^{} L = H(X)
$$

**最大熵**

离散型随机变量的最大熵为
<!-- max -->
$$
\max_{p_i} H(X) = \log |S|
$$

可以用梯度法直观感受，当所有分量的概率相等时，熵最大。

**联合熵**

联合概率
<!-- text -->
$$
p_{i, j} = \text{Pr}\lbrace X = x_i, Y = y_j\rbrace
$$

联合熵的定义：

$$
H(XY) = -\sum\limits_{i}^{}\sum\limits_{j}^{} p_{i, j} \log p_{i, j}
$$

**条件熵**

条件概率
<!-- text -->
$$
p_{i\mid j} = \text{Pr}\lbrace X = x_i \mid Y = y_j\rbrace
$$

条件熵的定义：

$$
H(X|Y) = -\sum\limits_{i}^{}\sum\limits_{j}^{} p_{i, j} \log p_{i\mid j}
$$

通过相关观测进行无损压缩，若观测到 $Y = \alpha_j$：

$$
\bar L(\alpha_j) = -\sum\limits_{i}^{}\sum\limits_{j}^{} p_{i\mid j} \log p_{i\mid j}
$$

于是

$$
\bar L =-\sum\limits_{j=1}^{N} \bar L(\alpha_j)p_j = -\sum\limits_{j=1}^{N}p_j\sum\limits_{i=1}^{M}p_{i|j} \log(p_{i|j}) = -\sum\limits_{i=1}^{M}\sum\limits_{j=1}^{N}p_{ij}\log(p_{i|j}) = H(X|Y)
$$

**链式法则**

$$
H(XY) = H(Y) + H(X|Y)
$$

两个随机变量的联合不确定性＝一个随机变量的不确定性＋知道这个随机变量后另一个随机变量残余的不确定性

**互信息(Mutual Infomation)**

$$
\begin{align*}
    I(X;Y) &= H(X) + H(Y) - H(XY)\\
    &= H(X) - H(X|Y) \\
    &= H(Y) - H(Y|X)
\end{align*}
$$

互信息的物理意义

第一种理解：
- X的不确定度减去观测Y后X残存的不确定度
- 即：通过观测Y带来的帮助了解X的信息

第二种理解：
- Y的不确定度减去观测X后Y残存的不确定度
- 即：通过观测X带来的帮助了解Y的信息

若$X, Y$相互独立，记为$X\perp Y$，则$I(X;Y) = 0$，$H(X) = H(X|Y)$，$H(Y) = H(Y|X)$。观测一个随机变量完全无助于了解另一个随机变量。

* $H(XY) = H(X) + H(Y)$，总平均码长等于各自平均码长之和。

若$X = Y$，则$I(X;Y) = H(X) = H(Y)$，$H(X|Y) = H(Y|X) = 0$。观测一个随机变量完全了解另一个随机变量。

$H(XY) = H(Y) + H(X|Y) = H(Y)$，只需要编码其中一个即可。

$$
X \perp Y \leftrightarrow H(X + Y | X) = H(Y | X) = H(Y), H(X + Y, X) = H(Y , X)
$$


$$
H(X + X | X) = H(X | X) = 0, H(X + X, X) = H(X , X) = H(X)
$$

**信息传输的基本模型**
* 信息通道，简称信道（Channel）对于输入符号有随机扰动，本质上可用一组条件概率表示
* 限于物理条件，信宿只能观测信道输出 $Y$，由此了解其输入 $X$
* 通过观测Y可以获得的关于X的信息量是 $I(X;Y)$

**信息传输的优化**

目标：最大化发送端 $X$ 和接收方 $Y$ 的互信息

方法：
* 信道是由物理实现所决定的，无法控制
* 但是可以选择X的概率分布

因此有如下优化问题：

$$
p*_i = \argmax_{\sum_i p_i = 1, p_i \ge 0} I(X;Y)
$$

定义信道容量 $C = \argmax_{\sum_i p_i = 1, p_i \ge 0} I(X;Y)$

信道容量的物理意义
- 平均每个信道符号所能传的最大的信息量
- 或：单位时间内信道所传最大的信息量

优化问题的表达式

$$
p_i^* = \argmax_{\sum_i p_i = 1, p_i \ge 0} - \sum_i\sum_j p_i p_{j|i} \log \frac{\sum\limits_i p_i p_{j|i}}{p_{j|i}}
$$

信道容量不易计算

**对称二进制信道(BSC)**

- 一种典型信道模型
- 分析信道编码时有很多应用

利用互信息表达式

$$
\begin{align*}
    I(X;Y) &= H(Y) - H(Y | X)\\
    &= H(Y) - \sum_i p_i \left[-\sum_j p_{j|i} \log p_{j|i}\right]\\
    &= H(Y) - [-\varepsilon\log \varepsilon - (1 - \varepsilon)\log(1 - \varepsilon)]\\
    &\le 1 - [-\varepsilon\log \varepsilon - (1 - \varepsilon)\log(1 - \varepsilon)] = C\\
\end{align*}\\
Y \sim \begin{bmatrix}
    0 & 1\\
    1/2 & 1/2
\end{bmatrix}
$$

如果误码率 $\varepsilon = 0.5$，则信道容量为0, 传递不了信息。

如果误码率 $\varepsilon > 0.5$，继续增大差错率，反而可以提高信道容量。

**高斯信道**
<!-- **gauss** -->
$$
Y = X + N\\
f_N(n) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp\left(-\frac{(y - x)^2}{2\sigma^2}\right)
$$

**连续性随机变量的熵**

$$
H(X) = - \int\limits_{-\infty}^{\infty}p(x)\log p(x) \mathrm dx + \lim_{\Delta \rightarrow 0} \log \frac{1}{\Delta}
$$

我们只关心相对不确定性，定义微分熵

$$
h(X) = -\int\limits_{-\infty}^{\infty}p(x) \log p(x)\mathrm dx
$$

微分熵是对连续型变量相对不确定性的一种描述
- 其定义剔除了连续性或“精准要求”带来的困难，保
留了分布函数形状自身的特征
- 它说明用有限字符集合的字符串描述连续分布的随机
变量，则平均字符长度为无穷大
- 为了用有限长字符串描述信源，需要进行有损压缩，
从而带来失真，即原始信源和压缩结果之间的差异
- 失真测度包括：均方误差，绝对值误差，主观误差等
- 对于图像，视频和语音等连续信源的编码等均属于有
损压缩

联合熵：

$$
h(XY) = -\int\limits_{-\infty}^{\infty}p(x, y) \log p(x, y)\mathrm dx \mathrm{d}y
$$

条件熵：
$$
h(Y|X) = -\int\limits_{-\infty}^{\infty}p(x, y) \log p(y|x)\mathrm dx \mathrm{d}y
$$

互信息：

$$
\begin{align*}
    I(X;Y) &= h(X) + h(Y) - h(XY)\\
    &= h(X) - h(X|Y) \\
    &= h(Y) - h(Y|X)
\end{align*}
$$