---
title: 通网
date: 2023-09-18 11:14:16
tags: note
katex: true
---

## 信息论基础

### 离散随机变量的信息度量

$$
H(X) = \mathbf E\{H(X=x_i)\} = -\sum_i p_i \log p_i
$$

称为熵

单位：
* 2 (Bit)
* e (Nat)
* 10 (Hartely)

表示了信息描述的有效性极限

信源编码（Source Coding），通过信息的有效表示，提高通信的有效性。例如: Huffman 编码

离散随机变量的最大熵：$\max_{p_i} H(X) = \log|S|$

前缀码：任何码字都不是其他码字的前缀。前缀码保证了唯一可译码。是二叉树叶子节点。

**Kraft不等式**

对于信源字符集$\lbrace a_1, \dots, a_m\rbrace$，必满足：

$$
\sum\limits_{k=1}^{M}2^{-l(a_k)} \le 1
$$

同时，若上式成立，必存在码长分别为$𝑙(𝑎_𝑘)$的前缀码。


最小前缀码的平均码长：

$$
\min \bar L =\sum\limits_{i=1}^{M}p_il_i\\
s.t.\sum\limits_{i=1}^{M} 2^{-l_i} = 1
$$

由拉格朗日乘子法

$$
p_i = 2^{-l_i}, \bar L_{min} = -\sum\limits_{i=1}^{M}p_i \log p_i
$$

记 $H(X) = -\sum p_i\log p_i$ .一般的，上下界为$H(X) \le \bar L  \lt H(X) + 1$.

如果我们将$k$个独立同分布的信源符号 $x_1, \dots, x_k$堪称一个，对整体应用前缀码编码：

$$
\begin{align*}
    H(X_1, \dots, X_k) &= -\sum P(x_1, \dots, x_k) \log P(x_1, \dots, x_k)\\
    &= -\sum P(x_1, \dots, x_k) [\log P(x_1) +  \dots +  \log(x_k)]\\
    &= -\sum P(x_1)\log (x_1) - \dots - \sum P(x_k)\log (x_k)\\
    &= -kH(X)
\end{align*}
$$

直观：对长度为$𝑛$的$M$种信源符号序列，$𝑥_𝑖$出现的次数$≈𝑛𝑝_𝑖$

典型序列应满足上述分布，否则就“小众”“非典型”

典型的个数 # $≈ \frac{𝒏!}{(𝑛𝑝_1) !… (𝑛𝑝_M) !}$

平均每个信源符号可以用 $L = \frac{1}{n}\log\left(\frac{𝒏!}{(𝑛𝑝_1) !… (𝑛𝑝_M) !}\right)$ 个 bit 来表达。

通过 Stirling 公式可以得出 L的上下界：

$$

$$

故

$$
\lim\limits_{n\rightarrow \infty}^{} L = H(X)
$$

**最大熵**

离散型随机变量的最大熵为
<!-- max -->
$$
\max_{p_i} H(X) = \log |S|
$$

可以用梯度法直观感受，当所有分量的概率相等时，熵最大。

**联合熵**

联合概率
<!-- text -->
$$
p_{i, j} = \text{Pr}\lbrace X = x_i, Y = y_j\rbrace
$$

联合熵的定义：

$$
H(XY) = -\sum\limits_{i}^{}\sum\limits_{j}^{} p_{i, j} \log p_{i, j}
$$

**条件熵**

条件概率
<!-- text -->
$$
p_{i\mid j} = \text{Pr}\lbrace X = x_i \mid Y = y_j\rbrace
$$

条件熵的定义：

$$
H(X|Y) = -\sum\limits_{i}^{}\sum\limits_{j}^{} p_{i, j} \log p_{i\mid j}
$$

通过相关观测进行无损压缩，若观测到 $Y = \alpha_j$：

$$
\bar L(\alpha_j) = -\sum\limits_{i}^{}\sum\limits_{j}^{} p_{i\mid j} \log p_{i\mid j}
$$

于是

$$
\bar L =-\sum\limits_{j=1}^{N} \bar L(\alpha_j)p_j = -\sum\limits_{j=1}^{N}p_j\sum\limits_{i=1}^{M}p_{i|j} \log(p_{i|j}) = -\sum\limits_{i=1}^{M}\sum\limits_{j=1}^{N}p_{ij}\log(p_{i|j}) = H(X|Y)
$$

**链式法则**

$$
H(XY) = H(Y) + H(X|Y)
$$

两个随机变量的联合不确定性＝一个随机变量的不确定性＋知道这个随机变量后另一个随机变量残余的不确定性

**互信息(Mutual Infomation)**

$$
\begin{align*}
    I(X;Y) &= H(X) + H(Y) - H(XY)\\
    &= H(X) - H(X|Y) \\
    &= H(Y) - H(Y|X)
\end{align*}
$$

互信息的物理意义

第一种理解：
- X的不确定度减去观测Y后X残存的不确定度
- 即：通过观测Y带来的帮助了解X的信息

第二种理解：
- Y的不确定度减去观测X后Y残存的不确定度
- 即：通过观测X带来的帮助了解Y的信息

若$X, Y$相互独立，记为$X\perp Y$，则$I(X;Y) = 0$，$H(X) = H(X|Y)$，$H(Y) = H(Y|X)$。观测一个随机变量完全无助于了解另一个随机变量。

* $H(XY) = H(X) + H(Y)$，总平均码长等于各自平均码长之和。

若$X = Y$，则$I(X;Y) = H(X) = H(Y)$，$H(X|Y) = H(Y|X) = 0$。观测一个随机变量完全了解另一个随机变量。

$H(XY) = H(Y) + H(X|Y) = H(Y)$，只需要编码其中一个即可。

$$
X \perp Y \leftrightarrow H(X + Y | X) = H(Y | X) = H(Y), H(X + Y, X) = H(Y , X)
$$


$$
H(X + X | X) = H(X | X) = 0, H(X + X, X) = H(X , X) = H(X)
$$

**信息传输的基本模型**
* 信息通道，简称信道（Channel）对于输入符号有随机扰动，本质上可用一组条件概率表示
* 限于物理条件，信宿只能观测信道输出 $Y$，由此了解其输入 $X$
* 通过观测Y可以获得的关于X的信息量是 $I(X;Y)$

**信息传输的优化**

目标：最大化发送端 $X$ 和接收方 $Y$ 的互信息

方法：
* 信道是由物理实现所决定的，无法控制
* 但是可以选择X的概率分布

因此有如下优化问题：

$$
p*_i = \argmax_{\sum_i p_i = 1, p_i \ge 0} I(X;Y)
$$

定义信道容量 $C = \argmax_{\sum_i p_i = 1, p_i \ge 0} I(X;Y)$

信道容量的物理意义
- 平均每个信道符号所能传的最大的信息量
- 或：单位时间内信道所传最大的信息量

优化问题的表达式

$$
p_i^* = \argmax_{\sum_i p_i = 1, p_i \ge 0} - \sum_i\sum_j p_i p_{j|i} \log \frac{\sum\limits_i p_i p_{j|i}}{p_{j|i}}
$$

信道容量不易计算

**对称二进制信道(BSC)**

- 一种典型信道模型
- 分析信道编码时有很多应用

利用互信息表达式

$$
\begin{align*}
    I(X;Y) &= H(Y) - H(Y | X)\\
    &= H(Y) - \sum_i p_i \left[-\sum_j p_{j|i} \log p_{j|i}\right]\\
    &= H(Y) - [-\varepsilon\log \varepsilon - (1 - \varepsilon)\log(1 - \varepsilon)]\\
    &\le 1 - [-\varepsilon\log \varepsilon - (1 - \varepsilon)\log(1 - \varepsilon)] = C\\
\end{align*}\\
Y \sim \begin{bmatrix}
    0 & 1\\
    1/2 & 1/2
\end{bmatrix}
$$

如果误码率 $\varepsilon = 0.5$，则信道容量为0, 传递不了信息。

如果误码率 $\varepsilon > 0.5$，继续增大差错率，反而可以提高信道容量。

**高斯信道**
<!-- **gauss** -->
$$
Y = X + N\\
f_N(n) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp\left(-\frac{(y - x)^2}{2\sigma^2}\right)
$$

### 连续性随机变量的熵

$$
H(X) = - \int\limits_{-\infty}^{\infty}p(x)\log p(x) \mathrm dx + \lim_{\Delta \rightarrow 0} \log \frac{1}{\Delta}
$$

我们只关心相对不确定性，定义微分熵

$$
h(X) = -\int\limits_{-\infty}^{\infty}p(x) \log p(x)\mathrm dx
$$

微分熵是对连续型变量相对不确定性的一种描述
- 其定义剔除了连续性或“精准要求”带来的困难，保
留了分布函数形状自身的特征
- 它说明用有限字符集合的字符串描述连续分布的随机
变量，则平均字符长度为无穷大
- 为了用有限长字符串描述信源，需要进行有损压缩，
从而带来失真，即原始信源和压缩结果之间的差异
- 失真测度包括：均方误差，绝对值误差，主观误差等
- 对于图像，视频和语音等连续信源的编码等均属于有
损压缩

### 多元随机变量的熵

联合熵：

$$
h(XY) = -\int\limits_{-\infty}^{\infty}p(x, y) \log p(x, y)\mathrm dx \mathrm{d}y
$$

条件熵：
$$
h(Y|X) = -\int\limits_{-\infty}^{\infty}p(x, y) \log p(y|x)\mathrm dx \mathrm{d}y
$$

互信息：

$$
\begin{align*}
    I(X;Y) &= h(X) + h(Y) - h(XY)\\
    &= h(X) - h(X|Y) \\
    &= h(Y) - h(Y|X)
\end{align*}
$$

## 压缩编码

### 压缩编码的分类

* 无损压缩
    * 输入：数字序列
    * 输出：数字序列
    * 目的：使得平均长度更小
* 有损压缩
    * 输入：模拟信号
    * 输出：数字序列
    * 目的：实现数字传输

信号压缩编码的步骤：

* 抽样
* 量化
* 压缩编码

### 抽样

**连续时间信源的离散化**

![](../images/通网/2_1.jpg)

离散化的方式：在标准正交基上投影展开

$$
s(t) = \sum_k a_k\phi_k(t)\\
a_k = <s(t), \phi_k(t)>
$$

若 $s(t)$ 是时限信号（宽度 $T$），可以用傅里叶展开的系数作为离散化结果：$s(t) = \sum\limits_k a_k e^{2\pi jkt/T}$

若 $s(t)$ 是带限信号（带宽 $W$），可以在频域对 $\hat S(f)$ 做傅里叶展开：

$$
\hat S(f) = \sum\limits_k \alpha_k e^{2\pi jkf/(2W)}
$$

变换回时域时，得到 Nyquist 抽样定理：

$$
s(t) =\sum\limits_{k}^{}s(kT) \text{sinc}\left(\left(\frac{t}{T} - k\right)\pi\right), T = \frac{1}{2W}
$$

频域无混叠等价于时域无畸变

对于带通采样，有无混叠条件：

![](../images/通网/2_2.jpg)

可以推导得出：

$$
f_s = 2B\left(1 + \frac{M}{N}\right)\\
N = \left\lfloor\frac{f_H}{B}\right\rfloor\\
M = \left\lbrace\frac{f_H}{B}\right\rbrace\\
B = f_H - f_L
$$

![](../images/通网/2_3.jpg)

横轴为 $f_H/B$，纵轴为 $f_s$

**量化**

分层电平： $\lbrace x_k \lt x \le x_{k + 1} \rbrace$ 

重建/输出电平：$y_k$代表一个量化区间，用以重构信号时使用的电平值

量化函数： $y = Q(x)$, $y_k = Q\lbrace x_k \lt x \le x_{k + 1} \rbrace$

量化间隔： $\Delta_k = x_{k + 1} - x_{k}$

均匀量化 & 非均匀量化

均匀量化只对有界随机变量存在

### 量化

* 在此只讨论标量的量化
* 量化噪声： $q = x - y = x - Q(x)$ 
* 量化噪声是一个随机变量
* 方差 $\sigma_q^2 = \int_{-\infty}^{\infty}[x - Q(x)]^2p_x(x)\mathrm dx$
* 方差与输入信号分布有关，不存在普适的最佳量化方案

#### 量化噪声的计算

$$
\sigma_q^2 =\sum\limits_{k=1}^{L}\int_{x_k}^{x_{k + 1}}(x - y_k)^2p_x(x)\mathrm dx
$$

从较容易的情况着手
- 只考虑电平区间 $[-V,V]$ 之间的信号，并假设量化间隔很小，亦即分层电平很密
- 在实际情况中，信号的分布函数处处可导，此时每个量化区间内信号的条件分布为均匀分布

量化区间内，近似概率密度 $p_x(x) = \frac{P_k}{\Delta_k}$

密集分层的量化噪声近似

$$
\sigma_{qn}^2 = \sum\limits_{k=1}^{L}\int_{x_k}^{x_{k + 1}}(x - y_k)^2p_x(x)\mathrm dx = \sum\limits_{k=1}^{L}\frac{P_k}{\Delta_k}\int_{x_k}^{x_{k + 1}}(x - y_k)^2\mathrm dx = \frac{1}{12}\sum_{k = 1}^L P_k\Delta_k^2 = \frac{1}{12} \int_{-V}^{V}(\Delta_k)^2p_x(x)\mathrm dx
$$

若 $\Delta_k = \Delta$，

$$
\sigma_{qn}^2 = \frac{1}{12}\sum_k P_k\Delta_k^2 = \frac{\Delta_k^2}{12}
$$

计算量化结果做无损压缩后的比特数：

$$
H(Q(x)) = -\sum_k P_k \log P_k = \underbrace{- \int_{-\infty}^{\infty}p_x(x)\log p_x(x) \mathrm dx }_{h(X)} + \log \frac{1}{\Delta}
$$

由 $\Delta = \sqrt{12\sigma_{qn}^2} = 2\sigma_{qn}\sqrt{3}$，

$$
H(x) = h(x) + \log \frac{1}{2\sigma_{qn}\sqrt{3}}
$$

无损压缩的 bit 数为： $\tilde{R} = h(X) - \frac{1}{2}\log \sigma_{qn}^2 - 1.8$

对于均匀量化：

$$
\Delta_k = \frac{x_{max} - x_{min}}{L} = \frac{2x_{max}}{L}, \forall k
$$

于是

$$
\sigma_{qn}^2 = \frac{\Delta^2}{12} = \frac{x_{max}^2}{3L^2}
$$

这里的 $\sigma_{qn}^2$ 是正常量化噪声，仅仅是计算了信号落在 $[-x_{max}, x_{max}]$ 内的情况

如果信号落在 $[-x_{max}, x_{max}]$ 以外，就就近判断至两端的量化区间，产生过载噪声

$$
\sigma_{qo}^2 = \int_{x_{max}}^{\infty}(x - x_{max})^2p_x(x)\mathrm dx + \int^{-x_{max}}_{-\infty}(x + x_{max})^2p_x(x)\mathrm dx = 2\int_{x_{max}}^{\infty}(x - x_{max})^2p_x(x)\mathrm dx
$$

总噪声等于正常量化噪声加上过载噪声：

$$
\sigma_{qs}^2 = \sigma_{qn}^2 + \sigma_{qo}^2
$$

如果用 $R$ bit 编码：

$$
\Delta_k = \frac{2x_{max}}{L} = \frac{x_{max}}{2^{R - 1}}\\
\sigma_{qn}^2 = \frac{\Delta^2}{12}\int_{-x_{max}}^{x_{max}}p_x(x)\mathrm dx = \frac{x_{max}^2}{3 \times 2^{2R}}\int_{-x_{max}}^{x_{max}}p_x(x)\mathrm dx
$$

定义非过载信号功率：

$$
\sigma_s^2 = \int_{-x_{max}}^{x_{max}}x^2p_x(x)\mathrm dx
$$

当 $\int_{-x_{max}}^{x_{max}}p_x(x)\mathrm dx \rightarrow 1$， $SNR_q \approx \frac{\sigma_s^2}{x_{max}^2/(3 \times 2^{2R})} = 3 \times 2^{2R} \times \zeta^2$，这里定义 $\zeta = \frac{\sigma_s}{x_{max}}$ 为量化范围内信号的饱满程度。

对数单位下：

$$
SNR_q(dB) = 6.02R + 20\log_{10}(\zeta) + 4.77
$$

* 多一个 bit，$SNR_q$ 提升 $6.02dB$
* $\zeta$ 要在合理范围，$\zeta$ 过大时过载会严重劣化性能

#### 最优量化

均匀分布的最佳量化是区间等分，中点重建