<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="Blither Boom"><meta name="copyright" content="Blither Boom"><meta name="generator" content="Hexo 6.2.0"><meta name="theme" content="hexo-theme-yun"><title>Mobile-LLM 论文阅读 | Guo_Yun</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"blitherboom812.github.io","root":"/","title":["摸","🐟","人","的","日","常"],"version":"1.10.11","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}.","hits":"${hits} results found","hits_time":"${hits} results found in ${time} ms"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap"><meta name="description" content="综述Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security 数据集VideoGUI: A Benchmark for GUI Automation from Instructional Videos，把 GUI 能力划分为三层：High Level 表示高层次的操作意图。Middl">
<meta property="og:type" content="article">
<meta property="og:title" content="Mobile-LLM 论文阅读">
<meta property="og:url" content="https://blitherboom812.github.io/2024/06/09/Mobile-LLM/index.html">
<meta property="og:site_name" content="Guo_Yun">
<meta property="og:description" content="综述Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security 数据集VideoGUI: A Benchmark for GUI Automation from Instructional Videos，把 GUI 能力划分为三层：High Level 表示高层次的操作意图。Middl">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://blitherboom812.github.io/images/Mobile-LLM/1721718047655.png">
<meta property="og:image" content="https://blitherboom812.github.io/images/Mobile-LLM/1717937154526.png">
<meta property="og:image" content="https://blitherboom812.github.io/images/Mobile-LLM/1718009240031.png">
<meta property="og:image" content="https://blitherboom812.github.io/images/Mobile-LLM/1717937734857.png">
<meta property="og:image" content="https://blitherboom812.github.io/images/Mobile-LLM/1717940196994.png">
<meta property="og:image" content="https://blitherboom812.github.io/images/Mobile-LLM/1718003623779.png">
<meta property="og:image" content="https://blitherboom812.github.io/images/Mobile-LLM/1718003792356.png">
<meta property="og:image" content="https://blitherboom812.github.io/images/Mobile-LLM/1717939486804.png">
<meta property="og:image" content="https://blitherboom812.github.io/images/Mobile-LLM/1721962899191.png">
<meta property="og:image" content="https://blitherboom812.github.io/images/Mobile-LLM/1721962961205.png">
<meta property="og:image" content="https://blitherboom812.github.io/images/Mobile-LLM/1717937675502.png">
<meta property="og:image" content="https://blitherboom812.github.io/images/Mobile-LLM/1721792120662.png">
<meta property="og:image" content="https://blitherboom812.github.io/images/Mobile-LLM/1721792309928.png">
<meta property="og:image" content="https://blitherboom812.github.io/images/Mobile-LLM/1718009413946.png">
<meta property="og:image" content="https://blitherboom812.github.io/images/Mobile-LLM/1721924737611.png">
<meta property="article:published_time" content="2024-06-09T12:42:26.000Z">
<meta property="article:modified_time" content="2024-07-26T09:14:46.000Z">
<meta property="article:author" content="Blither Boom">
<meta property="article:tag" content="paper">
<meta property="article:tag" content="Mobile">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="survey">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blitherboom812.github.io/images/Mobile-LLM/1721718047655.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script><!-- hexo injector head_end start --><link rel="stylesheet" type="text/css" href="/css/latex.css"><link rel="stylesheet" type="text/css" href="/css/fonts.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="Table of Contents"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="Overview"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="Blither Boom"><img width="96" loading="lazy" src="/images/avatar.jpg" alt="Blither Boom"></a><div class="site-author-name"><a href="/about/">Blither Boom</a></div><span class="site-name">Guo_Yun</span><sub class="site-subtitle"></sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="Home"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="Archives"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">32</span></a></div><div class="site-state-item"><a href="/categories/" title="Categories"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">0</span></a></div><div class="site-state-item"><a href="/tags/" title="Tags"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">7</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="文档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="我的小伙伴们" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%BC%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">综述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.</span> <span class="toc-text">数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.</span> <span class="toc-text">模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RL-based-method"><span class="toc-number">3.1.</span> <span class="toc-text">RL-based method</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LLM-Agents"><span class="toc-number">3.2.</span> <span class="toc-text">LLM Agents</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Prompt-Engineering"><span class="toc-number">3.2.1.</span> <span class="toc-text">Prompt Engineering</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Multimodal"><span class="toc-number">3.2.2.</span> <span class="toc-text">Multimodal</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#UI-Understanding-and-Representation"><span class="toc-number">3.3.</span> <span class="toc-text">UI Understanding and Representation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LLM-RL"><span class="toc-number">3.4.</span> <span class="toc-text">LLM + RL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Code-based-Methods"><span class="toc-number">3.5.</span> <span class="toc-text">Code-based Methods</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Topics"><span class="toc-number">4.</span> <span class="toc-text">Topics</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Paper-Reading"><span class="toc-number">5.</span> <span class="toc-text">Paper Reading</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#20240725"><span class="toc-number">5.1.</span> <span class="toc-text">20240725</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#20240629"><span class="toc-number">5.2.</span> <span class="toc-text">20240629</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E5%B0%8F%E6%A8%A1%E5%9E%8B%E5%8D%8F%E5%90%8C"><span class="toc-number">5.3.</span> <span class="toc-text">大小模型协同</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Code-Policy"><span class="toc-number">5.4.</span> <span class="toc-text">Code Policy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#FFN-%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">5.5.</span> <span class="toc-text">FFN 的作用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9D%82%E9%A1%B9"><span class="toc-number">5.6.</span> <span class="toc-text">杂项</span></a></li></ol></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="https://blitherboom812.github.io/2024/06/09/Mobile-LLM/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="Blither Boom"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="Guo_Yun"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Mobile-LLM 论文阅读</h1><div class="post-meta"><div class="post-time"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="Created: 2024-06-09 20:42:26" itemprop="dateCreated datePublished" datetime="2024-06-09T20:42:26+08:00">2024-06-09</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-2-line"></span></span> <time title="Modified: 2024-07-26 17:14:46" itemprop="dateModified" datetime="2024-07-26T17:14:46+08:00">2024-07-26</time></div><div class="post-classify"><span class="post-tag"><a class="tag-item" href="/tags/paper/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">paper</span></a><a class="tag-item" href="/tags/Mobile/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">Mobile</span></a><a class="tag-item" href="/tags/LLM/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">LLM</span></a><a class="tag-item" href="/tags/survey/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">survey</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><p>Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security</p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>VideoGUI: A Benchmark for GUI Automation from Instructional Videos，把 GUI 能力划分为三层：High Level 表示高层次的操作意图。Middle Level 为用自然语言描述的单步操作，Atomic-action 为使用固定格式描述的准确操作。整个 pipeline 都是人标的，包括视频挑选，标注，验证，task 数量较少，86 个 full task，463 个 subtask。</p>
<p><img src="/../images/Mobile-LLM/1721718047655.png" alt="1721718047655" loading="lazy"></p>
<p>GUI-WORLD: A Dataset for GUI-oriented Multimodal LLM-based Agents，从 Youtube 上爬取视频数据+人类录频操作数据，提取关键帧，以及 QA 文本。关键帧的标注由人类完成，包括：执行的操作， 关键帧转移的目标，使用的软件或者网站，鼠标操作，键盘输入等。迷惑的是为什么人类操作还要录屏，既然最后是提取关键帧，那可以直接在人操作的时候提取关键帧？</p>
<p>谷歌： Onthe Effects of Data Scale on Computer Control Agents，在安卓场景下人标gui sft数据，有测试集</p>
<p>谷歌：Android in the Wild: A Large-Scale Dataset for Android Device Control</p>
<p>输入为 image 和 text instruction。包含 30 K 的数据，Google Apps 的占比最大。</p>
<p><img src="/../images/Mobile-LLM/1717937154526.png" alt="1717937154526" loading="lazy"></p>
<p>World of Bits (WoB): Use only keyboard &amp; mouse，输入为彩色图像，DOM 文档，请求。支持通过众包构造新的数据。</p>
<p>OpenAI Universe: Game, Web tasks，输入数据只有图像，操作键盘和鼠标。</p>
<p>AndroidEnv: A Reinforcement Learning Platform for Android: 在 android emulator 上运行的虚拟环境。</p>
<p>（MoTIF）A Dataset for Interactive Vision-Language Navigation with Unknown Command Feasibility：人类标注，有 feasible 字段。</p>
<p><img src="/../images/Mobile-LLM/1718009240031.png" alt="1718009240031" loading="lazy"></p>
<p>PIXELHELP: Mapping Natural Language Instructions to Mobile UI Action Sequences</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="RL-based-method"><a href="#RL-based-method" class="headerlink" title="RL-based method"></a>RL-based method</h3><p>中间步骤奖励非常重要。稀疏奖励下从0训练容易停滞不前，需要先行为克隆提升一定的性能，再执行 RL 算法。</p>
<p>泛化性差，不像 LLM 具有大量先验知识。</p>
<p>App-Buddy:  PPO based method, Interact with DOM.</p>
<p><img src="/../images/Mobile-LLM/1717937734857.png" alt="1717937734857" loading="lazy"></p>
<p>REINFORCEMENT LEARNING ON WEB INTERFACES USING WORKFLOW-GUIDED EXPLORATION： 稀疏奖励下从0训练容易停滞不前，需要先行为克隆提升一定的性能，再执行 RL 算法。但是 BC 方法容易过拟合，因此通过从示例中导出 workflow policy，再从 workflow policy 中采样新的 policy 的方法来获取新的 trace。</p>
<p><img src="/../images/Mobile-LLM/1717940196994.png" alt="1717940196994" loading="lazy"></p>
<h3 id="LLM-Agents"><a href="#LLM-Agents" class="headerlink" title="LLM Agents"></a>LLM Agents</h3><h4 id="Prompt-Engineering"><a href="#Prompt-Engineering" class="headerlink" title="Prompt Engineering"></a>Prompt Engineering</h4><p>Explore, Select, Derive, and Recall: Augmenting LLM with Human-like Memory for Mobile Task Automation，从 trace 中学习 subtasks，然后封装成 api，在后续执行任务时可以直接调用。也是纯文本推理。</p>
<p>M3A，SeeAct 都是采用截图标注，然后推理，速度会慢很多。</p>
<p>AutoDroid: LLM-powered Task Automation in Android (MobiCom 24)</p>
<p>Exploration + Execution 范式，基于 VH&#x2F;DOM 的 UI 表示。</p>
<p><img src="/../images/Mobile-LLM/1718003623779.png" alt="1718003623779" loading="lazy"></p>
<p>GPT4 成功率相当可观，finetune小模型的效果接近 GPT 3.5：</p>
<p><img src="/../images/Mobile-LLM/1718003792356.png" alt="1718003792356" loading="lazy"></p>
<p>离线部分：随机探索 + 生成 App Memory，对 UTG(UI Transition Graph) 进行分析，LLM 生成每个页面和每个 UI 元素的描述，并构建 embedding vector base。</p>
<p>在线部分：根据 embedding vector base 筛选 UI 元素，只留下那些重要的 UI 元素，并且利用 APP Memory 中的元素生成 Guide 辅助模型进行决策。</p>
<p>Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators</p>
<p>DroidBot-GPT: GPT-powered UI Automation for Android</p>
<p><img src="/../images/Mobile-LLM/1717939486804.png" alt="1717939486804" loading="lazy"></p>
<p>AppAgent: Multimodal Agents as Smartphone Users：利用 GPT4-V 进行探索+部署，支持 learn from demonstration。</p>
<h4 id="Multimodal"><a href="#Multimodal" class="headerlink" title="Multimodal"></a>Multimodal</h4><p>CogAgent: A Visual Language Model for GUI Agents，从结果来看 Auto GUI 还是挺能打的，700M 的 encoder-decoder 和 18B 的 CogAgent （CogVLM-17B 改造而来）不相上下。</p>
<p><img src="/../images/Mobile-LLM/1721962899191.png" alt="1721962899191" loading="lazy"></p>
<p>模型架构如下。设计上采用一个低分辨率的图像编码器来识别大部分 UI 元素和布局，高分辨率的编码器用来识别文字（这个有证据吗？仅仅有一个简单的消融实验）。比较反直觉的是，在 OCR 领域模型应该采用更小的隐藏层，而不像通用领域那样需要很大的隐藏层，所以高分辨率编码器反而参数更少，只有 0.30 B。而且，这东西每层都跟 Decoder 做特征融合，不直接使用高分辨率是因为 CogVLM 原来的架构就支持 224*224（经典数字），太大了在 Self Attention 阶段计算量会爆炸，所以这里通过压缩隐藏层大小 + Cross Attention 来降低计算量。</p>
<p>对齐方面，人标了2k条，然后把 Mind2Web 和 AITW 的数据拿过来用 GPT-4 标了 VQA 的数据集。</p>
<p>输出格式包括 Plan，Action，Operation，Operation 部分同样是让大模型生成操作和坐标数据，我依然很好奇到底 VLM 能不能理解坐标信息。</p>
<p><img src="/../images/Mobile-LLM/1721962961205.png" alt="1721962961205" loading="lazy"></p>
<p>Auto GUI: You Only Look at Screens: Multimodal Chain-of-Action Agents</p>
<p><img src="/../images/Mobile-LLM/1717937675502.png" alt="1717937675502" loading="lazy"></p>
<p>微软 Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators，大小模型协同，一个模型负责 planning，一个模型负责 executing，同时，在这个过程中实现隐私保护。这个过程会通过一个语言模型根据屏幕截图 + low level commands 来判断命令是否可执行、是否执行成功。当然，仅仅用 Decoder 做二元判断似乎有点浪费，感觉是个分类器就能做，用 bert 说不定更好。另外，Vision Encoder 在识别图片中文字这一块擅长吗？模型架构来自于 pix2seq，它原本是做目标检测的。</p>
<p><img src="/../images/Mobile-LLM/1721792120662.png" alt="1721792120662" loading="lazy"></p>
<p><img src="/../images/Mobile-LLM/1721792309928.png" alt="1721792309928" loading="lazy"></p>
<p>META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI</p>
<p><img src="/../images/Mobile-LLM/1718009413946.png" alt="1718009413946" loading="lazy"></p>
<h3 id="UI-Understanding-and-Representation"><a href="#UI-Understanding-and-Representation" class="headerlink" title="UI Understanding and Representation"></a>UI Understanding and Representation</h3><p>ActionBert: Leveraging User Actions for Semantic Understanding of User Interfaces：UI 理解，leveraged the temporal con<br>nections between UIs in a UI sequence to design their pretraining tasks</p>
<p>UIBert: Learning Generic Multimodal Representations for UI Understanding：self-alignment among different multimodal features in a single UI， use trainable lightweight encoders</p>
<h3 id="LLM-RL"><a href="#LLM-RL" class="headerlink" title="LLM + RL"></a>LLM + RL</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2306.03604">Enabling Intelligent Interactions between an Agent and an LLM: A Reinforcement Learning Approach</a></p>
<h3 id="Code-based-Methods"><a href="#Code-based-Methods" class="headerlink" title="Code-based Methods"></a>Code-based Methods</h3><h2 id="Topics"><a href="#Topics" class="headerlink" title="Topics"></a>Topics</h2><p>长序列的任务执行</p>
<p>多模态 GUI Agent</p>
<p>隐私保护</p>
<p>用户偏好&#x2F;个性化的 Agent</p>
<p>Proactive Agent</p>
<p>Show me how to do&#x2F;高效的策略更新方法</p>
<p>RL training in sparse reward</p>
<p>LLM as Reward Model</p>
<p>Action &amp; Workflow embedding</p>
<p>寻找 UI 的表征&#x2F; UI Understanding</p>
<h2 id="Paper-Reading"><a href="#Paper-Reading" class="headerlink" title="Paper Reading"></a>Paper Reading</h2><h3 id="20240725"><a href="#20240725" class="headerlink" title="20240725"></a>20240725</h3><p>ShareGPT4Video: Improving Video Understanding and Generation with Better Captions：视频数据理解，或许有助于 GUI Agent 的训练。提出了一个 Differential Sliding-window Captioning 的方案，让 GPT4o 根据上一帧和当前帧之间的差别来输出 Caption，最后加上一个 Summary 来描述整个视频的 pipeline。这里的关键是如何选关键帧，原文采用一个 CLIP Model，通过比较最后一帧和这一帧的相似性，选出差别较大的帧。</p>
<p>Learning Transferable Visual Models From Natural Language Supervision</p>
<p>Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers</p>
<p><img src="/../images/Mobile-LLM/1721924737611.png" alt="1721924737611" loading="lazy"></p>
<h3 id="20240629"><a href="#20240629" class="headerlink" title="20240629"></a>20240629</h3><p>Read Agent：利用分页解决大模型长文本表现差的问题（Lost in middle）。</p>
<h3 id="大小模型协同"><a href="#大小模型协同" class="headerlink" title="大小模型协同"></a>大小模型协同</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.17390">SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks</a>。用 BC 的小模型进行 fast thinking, 然后让大模型进行 slow thinking &#x2F; grounding.</p>
<h3 id="Code-Policy"><a href="#Code-Policy" class="headerlink" title="Code Policy"></a>Code Policy</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.16653">AdaPlanner: Adaptive Planning from Feedback with Language Models</a></p>
<p>Code as Policies: Language Model Programs for Embodied Control: 2209.07753, Use Code (Formal Language) in Embodied Agent.</p>
<h3 id="FFN-的作用"><a href="#FFN-的作用" class="headerlink" title="FFN 的作用"></a>FFN 的作用</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.14680">Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2304.14767">Dissecting Recall of Factual Associations in Auto-Regressive Language Models</a></p>
<h3 id="杂项"><a href="#杂项" class="headerlink" title="杂项"></a>杂项</h3><p>MatFormer：嵌套训练 FFN 参数，适配不同的设备。</p>
<p>SADMoE：对 weight 聚类，将双层 MLP 转为 MoE</p>
<p>Deja Vu：预测 attention 的激活值，动态分配空间</p>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>Post author: </strong>Blither Boom</li><li class="post-copyright-link"><strong>Post link: </strong><a href="https://blitherboom812.github.io/2024/06/09/Mobile-LLM/" title="Mobile-LLM 论文阅读">https://blitherboom812.github.io/2024/06/09/Mobile-LLM/</a></li><li class="post-copyright-license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> unless otherwise stated.</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/2024/07/15/mathmagic/" rel="prev" title="mathmagic"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">mathmagic</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/2024/04/16/OS/" rel="next" title="操作系统"><span class="post-nav-text">操作系统</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2025 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> Blither Boom</span></div><div class="powered"><span>Powered by <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> v6.2.0</span><span class="footer-separator">|</span><span>Theme - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.11</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a><script>function initMourn() {
  const date = new Date();
  const today = (date.getMonth() + 1) + "-" + date.getDate()
  const mourn_days = ["4-4","9-18","12-13"]
  if (mourn_days.includes(today)) {
    document.documentElement.style.filter = "grayscale(1)";
  }
}
initMourn();</script></body></html>