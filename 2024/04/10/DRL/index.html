<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="Blither Boom"><meta name="copyright" content="Blither Boom"><meta name="generator" content="Hexo 6.2.0"><meta name="theme" content="hexo-theme-yun"><title>Deep Reinforcement Learning | Guo_Yun</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...true?.options,
  });
});</script><link rel="icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"blitherboom812.github.io","root":"/","title":["Êë∏","üêü","‰∫∫","ÁöÑ","Êó•","Â∏∏"],"version":"1.10.11","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}.","hits":"${hits} results found","hits_time":"${hits} results found in ${time} ms"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap"><meta name="description" content="Policy GraidentÂ∏¶ÊùÉÈáçÁöÑÊ¢ØÂ∫¶‰∏ãÈôçÊñπÊ≥ï $$ \nabla_\theta J(\theta)&#x3D;\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(a_t|s_t)R(\tau)] $$  A2C$$ \Delta\theta&#x3D;\alpha\nabla_\theta(log\pi_\theta(s,a))\hat{q}_w(s,a)\\">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Reinforcement Learning">
<meta property="og:url" content="https://blitherboom812.github.io/2024/04/10/DRL/index.html">
<meta property="og:site_name" content="Guo_Yun">
<meta property="og:description" content="Policy GraidentÂ∏¶ÊùÉÈáçÁöÑÊ¢ØÂ∫¶‰∏ãÈôçÊñπÊ≥ï $$ \nabla_\theta J(\theta)&#x3D;\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(a_t|s_t)R(\tau)] $$  A2C$$ \Delta\theta&#x3D;\alpha\nabla_\theta(log\pi_\theta(s,a))\hat{q}_w(s,a)\\">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-04-10T05:34:06.000Z">
<meta property="article:modified_time" content="2024-04-17T07:06:59.000Z">
<meta property="article:author" content="Blither Boom">
<meta name="twitter:card" content="summary"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script><!-- hexo injector head_end start --><link rel="stylesheet" type="text/css" href="/css/latex.css"><link rel="stylesheet" type="text/css" href="/css/fonts.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="Table of Contents"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="Overview"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="Blither Boom"><img width="96" loading="lazy" src="/images/avatar.jpg" alt="Blither Boom"></a><div class="site-author-name"><a href="/about/">Blither Boom</a></div><span class="site-name">Guo_Yun</span><sub class="site-subtitle"></sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="Home"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="Archives"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">32</span></a></div><div class="site-state-item"><a href="/categories/" title="Categories"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">0</span></a></div><div class="site-state-item"><a href="/tags/" title="Tags"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">7</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="ÊñáÊ°£"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="ÊàëÁöÑÂ∞è‰ºô‰º¥‰ª¨" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Policy-Graident"><span class="toc-number">1.</span> <span class="toc-text">Policy Graident</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#A2C"><span class="toc-number">2.</span> <span class="toc-text">A2C</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Model-Based-RL"><span class="toc-number">3.</span> <span class="toc-text">Model-Based RL</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Model-Based-RL-1"><span class="toc-number">3.1.</span> <span class="toc-text">Model-Based RL</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Open-Loop-Planning"><span class="toc-number">3.1.1.</span> <span class="toc-text">Open Loop Planning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Trajectory-Optimization-with-Derivatives"><span class="toc-number">3.1.2.</span> <span class="toc-text">Trajectory Optimization with Derivatives</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Model-Free-RL"><span class="toc-number">3.2.</span> <span class="toc-text">Model-Free RL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Model-based-RL-with-a-policy"><span class="toc-number">3.3.</span> <span class="toc-text">Model-based RL with a policy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Value-Equivalent-Model"><span class="toc-number">3.4.</span> <span class="toc-text">Value-Equivalent Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Model-Base-RL-with-images"><span class="toc-number">3.5.</span> <span class="toc-text">Model-Base RL with images</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Imitation-Learning"><span class="toc-number">4.</span> <span class="toc-text">Imitation Learning</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="https://blitherboom812.github.io/2024/04/10/DRL/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="Blither Boom"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="Guo_Yun"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Deep Reinforcement Learning</h1><div class="post-meta"><div class="post-time"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="Created: 2024-04-10 13:34:06" itemprop="dateCreated datePublished" datetime="2024-04-10T13:34:06+08:00">2024-04-10</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-2-line"></span></span> <time title="Modified: 2024-04-17 15:06:59" itemprop="dateModified" datetime="2024-04-17T15:06:59+08:00">2024-04-17</time></div><div class="post-classify"></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><h2 id="Policy-Graident"><a href="#Policy-Graident" class="headerlink" title="Policy Graident"></a>Policy Graident</h2><p>Â∏¶ÊùÉÈáçÁöÑÊ¢ØÂ∫¶‰∏ãÈôçÊñπÊ≥ï</p>
<div>$$
\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}[\nabla_\theta\log\pi_\theta(a_t|s_t)R(\tau)]
$$</div>

<h2 id="A2C"><a href="#A2C" class="headerlink" title="A2C"></a>A2C</h2><div>$$
\Delta\theta=\alpha\nabla_\theta(log\pi_\theta(s,a))\hat{q}_w(s,a)\\
\Delta w=\beta\left(R(s,a)+\gamma\hat{q}_{w}(s_{t+1},a_{t+1})-\hat{q}_{w}(s_{t},a_{t})\right)\nabla_{w}\hat{q}_{w}(s_{t},a_{t})\\
$$</div>

<h2 id="Model-Based-RL"><a href="#Model-Based-RL" class="headerlink" title="Model-Based RL"></a>Model-Based RL</h2><h3 id="Model-Based-RL-1"><a href="#Model-Based-RL-1" class="headerlink" title="Model-Based RL"></a>Model-Based RL</h3><p>If we know the dynamics of some process:</p>
<p>Objective in a Stochastic World</p>
<p>The dynamics are stochastic</p>
<p>The expectation under these actions in such a stochastic world.</p>
<div>$$
\begin{aligned}p_\theta(\mathbf{s}_1,\ldots,\mathbf{s}_T\mid\mathbf{a}_1,\ldots,\mathbf{a}_T)&=p(\mathbf{s}_1)\prod_{t=1}^Tp(\mathbf{s}_{t+1}\mid\mathbf{s}_t,\mathbf{a}_t)\\\mathbf{a}_1,\ldots,\mathbf{a}_T&=\arg\max_{\mathbf{a}_1,\ldots,\mathbf{a}_T}E\left[\sum_tr(\mathbf{s}_t,\mathbf{a}_t)\mid\mathbf{a}_1,\ldots,\mathbf{a}_T\right]\end{aligned}
$$</div>

<p>It is suboptimal.</p>
<h4 id="Open-Loop-Planning"><a href="#Open-Loop-Planning" class="headerlink" title="Open Loop Planning"></a>Open Loop Planning</h4><p>Guess and check (Random Shooting)</p>
<ul>
<li>Pick action sequences uniformly in the action space</li>
<li>Calculate the Result</li>
</ul>
<p>Better: Cross Entropy Method</p>
<p>Example: MCTS</p>
<h4 id="Trajectory-Optimization-with-Derivatives"><a href="#Trajectory-Optimization-with-Derivatives" class="headerlink" title="Trajectory Optimization with Derivatives"></a>Trajectory Optimization with Derivatives</h4><p>LQR? Linear Quadratic Regulator</p>
<div>$$
x_{t+1} = Ax_t+ Bu_t\\
c(x_t, u_t) = x_t^TQx_t + u_t^TRu_t
$$</div>


<p>The Q and R are symmetric and ponlositive definite. If not positive, you may optimize the result into negative inf.</p>
<p>Value Iteration</p>
<h3 id="Model-Free-RL"><a href="#Model-Free-RL" class="headerlink" title="Model-Free RL"></a>Model-Free RL</h3><p>If the model is not known?</p>
<p>modelbased RL:</p>
<ul>
<li>base policy to collect dataset</li>
<li>learning dynamics model from dataset</li>
<li>plan through dynamics model and give actions</li>
<li>Execute the actions and add the result into data set</li>
</ul>
<p>Model predictive control (MPC)</p>
<ul>
<li>base policy to collect dataset</li>
<li>learning dynamics model from dataset</li>
<li>plan through dynamics model and give actions</li>
<li>only execute the first planned action</li>
<li>append the $(s, a, s^\prime)$ to dataset</li>
</ul>
<h3 id="Model-based-RL-with-a-policy"><a href="#Model-based-RL-with-a-policy" class="headerlink" title="Model-based RL with a policy"></a>Model-based RL with a policy</h3><p>Why Model based RL with a learned model?</p>
<ul>
<li>Data-efficiency<ul>
<li>Dont need to act in real world</li>
</ul>
</li>
<li>Multi-task with a model<ul>
<li>reuse the world model</li>
</ul>
</li>
</ul>
<p>But they are unstable and worse asymptotic performance.</p>
<ol>
<li>If the model biased toword the positive side<ul>
<li>the actions overfit to the learned model</li>
</ul>
</li>
<li>if the trajectory is really long<ul>
<li>Accumulated errors</li>
</ul>
</li>
</ol>
<p>To resolve 1: use uncertainty</p>
<p>Optimize towards expectation of rewards rather than rewards</p>
<p>Two types of uncertainty</p>
<ul>
<li>Aleatoric or statistical: The reality itself has uncertainty (e.g. Dice)</li>
<li>Epistemic or model uncertainty: You are uncertain about the true function</li>
</ul>
<p>If use output entropy, it can‚Äôt tell apart the type of uncertainty. We need to measure the epistemic uncertainty.</p>
<p>How to measure?</p>
<p>We use the collected data to train the model, maximize $\log(D|\theta)$ by changing $\theta$</p>
<p>Can we instead to measure $\log(\theta|D)$ ‚Äì the model uncertainty!</p>
<p>but it is rather intractable.</p>
<p>Model Ensemble!</p>
<p>Training multiple models, see if they agree with each other. We have to make the models different(variant).</p>
<p>The randomness and SGD is enough to make the models different.</p>
<ul>
<li>Every time drag one model and give actions</li>
<li>calculate the reward</li>
<li>add the data into dataset and update policy</li>
</ul>
<p>To resolve 2 (long rollouts can be error-prone), we can always use short rollouts.</p>
<p>combine the real and model data to improve policy</p>
<p>Example: DYNA-style MBRL</p>
<p>Also can try Baysian Neural Networks.</p>
<h3 id="Value-Equivalent-Model"><a href="#Value-Equivalent-Model" class="headerlink" title="Value-Equivalent Model"></a>Value-Equivalent Model</h3><p>You dont have to stimulate the world, just simplify the value fuction ensuring to keep the value is approximately the same with the real one.</p>
<p>Use mean square error.</p>
<h3 id="Model-Base-RL-with-images"><a href="#Model-Base-RL-with-images" class="headerlink" title="Model-Base RL with images"></a>Model-Base RL with images</h3><h2 id="Imitation-Learning"><a href="#Imitation-Learning" class="headerlink" title="Imitation Learning"></a>Imitation Learning</h2><p>Accumulate Error and Covariate Shiift</p>
<p>DAgger:</p>
<ul>
<li>Train a policy from human data $D$</li>
<li>Run the policy to get dataset $D_\pi$</li>
<li>Ask human to label $D_\pi$ with actions $a_t$</li>
<li>Aggregate: $D \larr D \cup D_\pi$</li>
</ul>
<p>Techniques: Dataset Resampling &#x2F; Reweighting</p>
<p>Techniques: Pre-Trained Models to extract representations</p>
<p>MSE gives the mean value, while the cross-entropy gives the probability. If a task has a probability with 50% left, 50% right, the MSE will give an answer ‚Äúgo forward‚Äù.</p>
<p>Leverage Demonstrations in DRL</p>
<p>DQfD: Deep Q-Learning from Demonstrations</p>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>Post author: </strong>Blither Boom</li><li class="post-copyright-link"><strong>Post link: </strong><a href="https://blitherboom812.github.io/2024/04/10/DRL/" title="Deep Reinforcement Learning">https://blitherboom812.github.io/2024/04/10/DRL/</a></li><li class="post-copyright-license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> unless otherwise stated.</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/2024/04/16/SolidPhysics/" rel="prev" title="Âõ∫‰ΩìÁâ©ÁêÜ"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">Âõ∫‰ΩìÁâ©ÁêÜ</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/2024/03/19/hello-world/" rel="next" title="Hello World"><span class="post-nav-text">Hello World</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 ‚Äì 2025 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> Blither Boom</span></div><div class="powered"><span>Powered by <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> v6.2.0</span><span class="footer-separator">|</span><span>Theme - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.11</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a><script>function initMourn() {
  const date = new Date();
  const today = (date.getMonth() + 1) + "-" + date.getDate()
  const mourn_days = ["4-4","9-18","12-13"]
  if (mourn_days.includes(today)) {
    document.documentElement.style.filter = "grayscale(1)";
  }
}
initMourn();</script></body></html>