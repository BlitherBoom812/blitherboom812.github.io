<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="Blither Boom"><meta name="copyright" content="Blither Boom"><meta name="generator" content="Hexo 6.2.0"><meta name="theme" content="hexo-theme-yun"><title>DigiRL | Guo_Yun</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...true?.options,
  });
});</script><link rel="icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"blitherboom812.github.io","root":"/","title":["摸","🐟","人","的","日","常"],"version":"1.10.11","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}.","hits":"${hits} results found","hits_time":"${hits} results found in ${time} ms"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap"><meta name="description" content="DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement LearningLLM (VLM) +RL 的完美之作！在 Pretraining 的基础上进行 Offline RL，然后在执行任务时通过 Online RL 更新参数，特色是能够从多轮交互中学习、更新参数，而不是通过专家知识微调一个单">
<meta property="og:type" content="article">
<meta property="og:title" content="DigiRL">
<meta property="og:url" content="https://blitherboom812.github.io/2024/07/17/DigiRL/index.html">
<meta property="og:site_name" content="Guo_Yun">
<meta property="og:description" content="DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement LearningLLM (VLM) +RL 的完美之作！在 Pretraining 的基础上进行 Offline RL，然后在执行任务时通过 Online RL 更新参数，特色是能够从多轮交互中学习、更新参数，而不是通过专家知识微调一个单">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://blitherboom812.github.io/images/Mobile-LLM/1721230554686.png">
<meta property="og:image" content="https://blitherboom812.github.io/images/Mobile-LLM/1721231007829.png">
<meta property="og:image" content="https://blitherboom812.github.io/images/DigiRL/1721232302611.png">
<meta property="og:image" content="https://blitherboom812.github.io/images/DigiRL/1721316686785.png">
<meta property="og:image" content="https://blitherboom812.github.io/images/DigiRL/1721316668685.png">
<meta property="og:image" content="https://blitherboom812.github.io/images/DigiRL/1721554892738.png">
<meta property="og:image" content="https://blitherboom812.github.io/images/DigiRL/1721555459238.png">
<meta property="og:image" content="https://blitherboom812.github.io/images/DigiRL/1721555695196.png">
<meta property="article:published_time" content="2024-07-17T15:53:27.000Z">
<meta property="article:modified_time" content="2024-07-21T10:17:39.000Z">
<meta property="article:author" content="Blither Boom">
<meta property="article:tag" content="paper">
<meta property="article:tag" content="Mobile">
<meta property="article:tag" content="RL">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blitherboom812.github.io/images/Mobile-LLM/1721230554686.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script><!-- hexo injector head_end start --><link rel="stylesheet" type="text/css" href="/css/latex.css"><link rel="stylesheet" type="text/css" href="/css/fonts.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="Table of Contents"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="Overview"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="Blither Boom"><img width="96" loading="lazy" src="/images/avatar.jpg" alt="Blither Boom"></a><div class="site-author-name"><a href="/about/">Blither Boom</a></div><span class="site-name">Guo_Yun</span><sub class="site-subtitle"></sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="Home"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="Archives"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">33</span></a></div><div class="site-state-item"><a href="/categories/" title="Categories"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">0</span></a></div><div class="site-state-item"><a href="/tags/" title="Tags"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">7</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="文档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="我的小伙伴们" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#DigiRL-Training-In-The-Wild-Device-Control-Agents-with-Autonomous-Reinforcement-Learning"><span class="toc-number">1.</span> <span class="toc-text">DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RL-%E7%AE%97%E6%B3%95%E9%83%A8%E5%88%86"><span class="toc-number">2.</span> <span class="toc-text">RL 算法部分</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="toc-number">3.</span> <span class="toc-text">模型架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Instruction-and-Step-Level-Value-Functions"><span class="toc-number">3.1.</span> <span class="toc-text">Instruction and Step Level Value Functions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Actor"><span class="toc-number">3.2.</span> <span class="toc-text">Actor</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%84%E6%B5%8B%E9%83%A8%E5%88%86"><span class="toc-number">4.</span> <span class="toc-text">评测部分</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C%E5%92%8C%E5%88%86%E6%9E%90"><span class="toc-number">5.</span> <span class="toc-text">消融实验和分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%94%99%E8%AF%AF%E5%88%86%E6%9E%90"><span class="toc-number">5.1.</span> <span class="toc-text">错误分析</span></a></li></ol></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="https://blitherboom812.github.io/2024/07/17/DigiRL/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="Blither Boom"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="Guo_Yun"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">DigiRL</h1><div class="post-meta"><div class="post-time"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="Created: 2024-07-17 23:53:27" itemprop="dateCreated datePublished" datetime="2024-07-17T23:53:27+08:00">2024-07-17</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-2-line"></span></span> <time title="Modified: 2024-07-21 18:17:39" itemprop="dateModified" datetime="2024-07-21T18:17:39+08:00">2024-07-21</time></div><div class="post-classify"><span class="post-tag"><a class="tag-item" href="/tags/paper/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">paper</span></a><a class="tag-item" href="/tags/Mobile/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">Mobile</span></a><a class="tag-item" href="/tags/RL/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">RL</span></a><a class="tag-item" href="/tags/LLM/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">LLM</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><h2 id="DigiRL-Training-In-The-Wild-Device-Control-Agents-with-Autonomous-Reinforcement-Learning"><a href="#DigiRL-Training-In-The-Wild-Device-Control-Agents-with-Autonomous-Reinforcement-Learning" class="headerlink" title="DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning"></a>DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning</h2><p>LLM (VLM) +RL 的完美之作！在 Pretraining 的基础上进行 Offline RL，然后在执行任务时通过 Online RL 更新参数，特色是能够从多轮交互中学习、更新参数，而不是通过专家知识微调一个单步交互模型。实验也做得非常详细。不过 RL 的部分我还看不太懂，需要继续学习。</p>
<p><img src="/../images/Mobile-LLM/1721230554686.png" alt="1721230554686" loading="lazy"></p>
<p>Action Space 定义为点击、滑动、输入、home 键、返回等操作，参数包含屏幕上的归一化 (x, y) 坐标。另外引入的一个设定是界面的随机性，界面可能有随时更新、加载中、小广告、身份识别等干扰出现。</p>
<p><img src="/../images/Mobile-LLM/1721231007829.png" alt="1721231007829" loading="lazy"></p>
<h2 id="RL-算法部分"><a href="#RL-算法部分" class="headerlink" title="RL 算法部分"></a>RL 算法部分</h2><p>首先本文将手机操作问题建模为一个 MDP 过程。</p>
<p>然后采用了 AWR 算法，计算策略梯度。似乎用到了蒙特卡洛？</p>
<p>Instruction-Level Value Function 有什么作用？这里似乎是说 task set 的难度方差太大，需要用难度适中的数据去训练 actor model，所以提出了则个 Value Function 来过滤一部分数据。</p>
<p><img src="/../images/DigiRL/1721232302611.png" alt="1721232302611" loading="lazy"></p>
<p>首先，定义 Advantage 为一个状态的 Q 值（按照当前策略，未来所有步的期望值）减去 Value function：</p>

$$
A^\pi(s_h,a_h,c)=Q^\pi(s_h,a_h,c)-V^\pi(s_h,c).
$$


<p>AWR 算法的 Actor 优化目标是一个加权的 MLE：</p>

$$
\arg\max_\pi\mathbb{E}_\nu\left[\log\pi(a|s,c)\cdot\exp\left(A(s,a,c)/\beta\right)\right]
$$


<p>在论文中没有用这个公式，而是用 hard filtering 替代：</p>

$$
\mathcal{L}(\pi)=-\mathbb{E}_{\mathrm{filter}(\nu)}[\log\pi(a|s,c)].
$$


<p>Step Advantage 的估计采用的是 GAE (Generalized Advantage Estimation) 的简化版：</p>

$$
A^{\mathrm{step}}(s_h,a_h,c):=\lambda^{H-h}r(s_H,a_H,c)+(1-\lambda^{H-h}r(s_H,a_H,c))(V^{\mathrm{step}}(s_{h+1},c)+r(s_h,a_h,c)-V^{\mathrm{step}}(s_h,c))
$$


<p>之所以可以这么做，是因为任务的奖励只有在最后一步有，成功了是1，失败了是0，第一项是高方差的 MC，第二项是高偏差的估计器。随着 h 增加，第一项的权重越来越大，第二项权重则越来越小。这两项综合起来有利于均衡方差与偏差。</p>
<p>此外，我们还要定义 Instruct Advantage，用来评估一条 traj 的学习价值：</p>
<p><img src="/../images/DigiRL/1721316686785.png" alt="1721316686785" loading="lazy"></p>
<p>个人理解，traj 的学习价值取决于 traj 奖励值和指令平均奖励的差。</p>
<p>traj 的奖励值就是当前这条路径拿到的奖励总和。</p>
<p>“指令平均奖励”就是当前指令下获得奖励的期望值，取决于任务的难易程度，因为它是对该任务所有 traj 的平均，任务越难，奖励更难拿到，指令平均奖励越低。</p>
<p>那么，traj 的奖励值越高，traj 的学习价值就越高，直觉上即成功的任务（奖励为1）有学习价值，但是失败的任务（奖励为0）不值得学习；</p>
<p>指令平均奖励越高，traj 的学习价值越低，这是因为指令平均奖励高，意味着这个任务很简单，不值得学习。</p>
<p>关于估计器的训练，采用的是交叉熵损失而不是传统的 MSE 损失，这是因为交叉熵损失通常在 transformer 架构上更好用：</p>
<p><img src="/../images/DigiRL/1721316668685.png" alt="1721316668685" loading="lazy"></p>
<p>而且巧妙的点在于这个任务的奖励只有 1 或 0，因此估计器其实可以看作一个二分类，交叉熵就只有两项。</p>
<p>最终的 pipeline：</p>
<ul>
<li>训练 Step-level 和 Instruct-level 的估计器 V；</li>
<li>用估计器 V 计算学习价值 A，通过学习价值过滤掉一部分无效 traj 和 step；<ul>
<li>具体而言，Instruct-level 选取 top-p 条 traj；</li>
<li>Step-level 选取阈值大于 1&#x2F;H 的 step.</li>
</ul>
</li>
<li>在过滤后的 traj 上采用 MLE 准则训练模型。</li>
</ul>
<h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p>在 AutoUI-Base 的基础上进行训练，固定 image encoder 不动。</p>
<h3 id="Instruction-and-Step-Level-Value-Functions"><a href="#Instruction-and-Step-Level-Value-Functions" class="headerlink" title="Instruction and Step Level Value Functions"></a>Instruction and Step Level Value Functions</h3><p>输入：采用 image encoder 与 RoBERTa 分别对界面截图和指令进行 embedding，拼接起来。</p>
<p>用 2 层的 MLP 来预测 Value function。</p>
<h3 id="Actor"><a href="#Actor" class="headerlink" title="Actor"></a>Actor</h3><p>在离线学习阶段，通过运行原始的 AutoUI-Base 来采集 traj。在 offline 阶段，跳过了 instruction-level filtering，用所有的 instruction 来训练，用以充分地利用数据。</p>
<p>Decoder 具体要输出什么？是文本吗？那数值是怎么处理的？大模型能很好地理解数值吗？（9.11 &gt; 9.8？？）</p>
<p>答：按照 Auto-GUI 的说法，确实是文本，点击屏幕需要用到一个 0~1 之间的 4 位小数。感觉这样肯定有很多问题，例如语言模型怎么理解空间关系和数值关系？它是不是只是背下了 UI 的数字信息？UI 的理解和识别工作应该已经有很多了。</p>
<p>如果要验证这点，可以设计一个空间方位相关的 Instruction，例如让它点击按钮 A 上方的那个按钮，语言模型不可能回答出来。或者，将按钮平移一段距离，看模型对于按钮位置的预测是否出现了一致的平移？</p>
<p>我比较相信的一种可能是，Image Encoder 在编码时确实把 UI 的位置坐标信息和语义信息编码进了 Embedding 中，并且在 Decoding 阶段确实被识别了出来，这样想倒也是自洽的。如何验证？</p>
<h2 id="评测部分"><a href="#评测部分" class="headerlink" title="评测部分"></a>评测部分</h2><p>评测采用的是 Gemini-1.5-pro，据论文报告结果和人类的评估接近。评测标准是通过一个端到端的观察，判断是否完成了任务。</p>
<h2 id="消融实验和分析"><a href="#消融实验和分析" class="headerlink" title="消融实验和分析"></a>消融实验和分析</h2><h3 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h3><p>整体上，所有测试的 Agent 存在三种错误：</p>
<ul>
<li>很难从错误中纠正</li>
<li>在中途卡住，例如搜索之后不知道点搜索按钮</li>
<li>到达了错误的目标，例如走错了网购网站</li>
</ul>
<p>错误1、3貌似都是任务规划层面的问题，而不是操作执行的问题，模型之所以出错是因为它没有产生正确的意图，而不是它不理解 UI 界面本身。错误 2 是一个 UI 理解的问题。也就是说，作者可能认为模型在 planning 方面的问题是最主要的。当然作者并没有严格区分这二者，因为训练时就是以多轮操作执行为目标进行训练的。当然，这二者可能确实是无法分开的，因为不理解 UI 界面的逻辑，就无法进行正确的任务规划。</p>
<p>作者认为，预训练模型之所以为什么会出现这样的错误，是因为它们无法应对动态性强的环境，进入了之前从未见过的界面时候就不知道怎么从其中出来了。相反，DigiRL 通过收集 Rollouts 数据，学会了从错误中学习，从而纠正自己的错误。</p>
<p>我觉得这个说法有点牵强，首先这个只能解释错误类型 1、3，不能解释类型2。并不是所有模型都在预训练时接受了特定的 APP 界面相关的数据，例如 GPT-4V，AITW 的手机界面对于他来说，都是“没见过”的界面，而且点搜索按钮这件事没有太多的动态性，这跟动态性没什么关系，这个例子根本上说明的是它不理解 UI 界面语言：</p>
<p><img src="/../images/DigiRL/1721554892738.png" alt="1721554892738" loading="lazy"></p>
<p>它没有理解界面的内容，认为搜索框右边那个×号是搜索的意思。实际上，这个任务确实有些 tricky，因为浏览器是不提供搜索键的，回车键在这个场景中充当了搜索键的作用。GPT-4V 首先应该理解到右下角那个箭头指的是回车，第二步是认识到回车键在浏览器场景下的搜索意义。</p>
<p>我倾向于 GPT-4V 没有理解第一步，也就是图标本身表示回车，可能它会认为这就是一个箭头。第二步对这样的模型而言应该是非常简单就能转过来的。而且更致命的是，这个 UI 里回车根本没标上数字，那 GPT-4V 即使知道要点回车，它也没有选择的机会。比如下面这个例子：</p>
<p><img src="/../images/DigiRL/1721555459238.png" alt="1721555459238" loading="lazy"></p>
<p>很好笑的是，它知道“键盘上有一个搜索按钮”，但是这个按钮没标数字，它只好点击了和那个回车键距离最近的 24 号，然后它就成功点进了广告。排除广告干扰这件事本身可能相当重要，需要专门的对齐工作。</p>
<p>这里的例子全是 GPT-4V 的，感觉没什么看头，毕竟是附录，可能写得没那么认真。</p>
<p><img src="/../images/DigiRL/1721555695196.png" alt="1721555695196" loading="lazy"></p>
<p>网购这件事可能相对比较困难一点，可以看到主要的困难都是在点击按钮和走错界面上（除了 Gemini 完全无法正常 work，hh），模型产生了正确的意图，但是点错了地方，即使是论文自己的模型也没能幸免，毕竟 Image Encoder 是固定不动的。也就是说模型对于 UI 操作的理解并不充分。</p>
<p>总体而言，错误类型可以分成 3 类：</p>
<ul>
<li>任务规划错误：Recover from mistakes, Fail at all</li>
<li>UI 理解错误：包括两个子类：<ul>
<li>不可交互，仅可阅读的 UI 元素意义理解错误：Quit Early, wrong page</li>
<li>可交互 UI 元素的理解错误：Click on link or type</li>
</ul>
</li>
</ul>
<p>DigiRL 的工作，主要解决了任务规划方面的错误，因为模型可以接受奖励信号，判断整条 trace 是否做错了。仔细想想这对于 GPT-4V 是不公平的，因为没人告诉 4V 是否做错了，4V 也不可能接受 1000 条 traj 的 Online Training，信息量自然少了很多。</p>
<p>但是对比而言，模型主要的错误可能还是在 UI 理解上。模型对于 UI 元素理解的能力还是很差，像 4V 这种模型甚至也会有理解错误。VLM 的能力还任重道远。</p>
<p>因此其实后续工作可以探讨的还有很多：</p>
<ul>
<li>UI 理解与交互，个人认为是目前手机 Agent 最关键的点；</li>
<li>任务规划，这个就接着 DigiRL 的思路，RL 的方法虽然成功实现了 Online Training，但是先验知识利用不足，需要 1k 量级的数据去学会一个没见过的任务，能不能做到像 ChatGPT 那样 ICL 就能直接拿下一个新任务？（这个可能很难，需要大量的标注数据）</li>
<li>异常页面处理，包括怎么鉴别误导页面，怎么从广告、弹窗、警告等非常规界面中退出，这个也许很新颖，至少我想不到有什么工作在做这个，但是这个事情可能对于 wild env 的 Agent 非常重要。</li>
</ul>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>Post author: </strong>Blither Boom</li><li class="post-copyright-link"><strong>Post link: </strong><a href="https://blitherboom812.github.io/2024/07/17/DigiRL/" title="DigiRL">https://blitherboom812.github.io/2024/07/17/DigiRL/</a></li><li class="post-copyright-license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> unless otherwise stated.</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/2024/07/25/llama3-1/" rel="prev" title="Llama3.1 report"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">Llama3.1 report</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/2024/07/15/mathmagic/" rel="next" title="mathmagic"><span class="post-nav-text">mathmagic</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2026 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> Blither Boom</span></div><div class="powered"><span>Powered by <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> v6.2.0</span><span class="footer-separator">|</span><span>Theme - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.11</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a><script>function initMourn() {
  const date = new Date();
  const today = (date.getMonth() + 1) + "-" + date.getDate()
  const mourn_days = ["4-4","9-18","12-13"]
  if (mourn_days.includes(today)) {
    document.documentElement.style.filter = "grayscale(1)";
  }
}
initMourn();</script></body></html>